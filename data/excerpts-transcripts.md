# 19e

> it or something. But, yeah, then I thought using a large data set and treat it as
some kind of population and sample from that 25 subjects would be a better
way to maybe avoid detection. Yeah, that was my reasoning.

> looked at the mean of the means, the standard deviation of the means, the mean
of the standard deviations of the - standard deviations. And then, I thought,
well, what else could you guys use to detect fraud and then I thought, maybe
there is something known in the Stroop task about the correlation between
those measures. Probably, there is - I don’t know. So, I also looked at the
correlation between those four measures, right - mean and standard deviation
from the two condition. And then I generated a new 121 sized data set with
those characteristics. So, just in R - I just generated a correlated data set with

> template. Then, manually, I checked for crazy outliers - like I don’t know -
maybe, there were response times of below 100 milliseconds and that would of
course be not possible. But there weren’t any. So, then I decided that that

> P: Not really. I am just very curious as to the methods that other people used.
I mean, I was thinking about, maybe, I should use like an LCA model or an
LBA model to generate data. And I thought: No, I am way overthinking it. It
doesn’t have to be that hard, I guess.

> P: Yeah, because it is, I guess, fun to do something that is really not allowed
in science. And yeah, I mean, I think [REDACTED] also did it and was like

> that sounds like a good challenge. Wonder if I can beat [REDACTED] with
generating data. So, yeah, it seemed like a fun challenge.

> were interested in participating but I think we are too competitive to actually
discuss our methods with each other.

# 1se

>P: No. But I did recruit one of the postdocs.

>P: To discuss how we could do this.

>P: So, I didn’t do this on my own. I recruited additional help.

---

> congruent trial and [?] some 2-300 milliseconds longer. So, there is kind of a
range of plausibility that you can extract from the literature. If the latencies
would be completely off - like by a factor(?) of 5 or 10 - then something surely
must be a miss. So, distribution, absolute values, and the lack of outliers - this
would be, I think, hallmarks of fabrication. That’s what I look for when I get
students’ work. So, that is probably why I come up with these.

---

> P: Of course. We started out with great reluctance not wanting to fake data but
once we accepted the mission we really wanted to make sure that you couldn’t
detect our playing this game. So, yes, we tried to think about how we could
make the data look real.

---

> P: Yeah, we took into account the fact that the absolute latency has a relationship
with the standard deviation. If you are very slow then the standard deviation
will be a little bit longer. So, there was a correlation between the mean and the
standard deviation introduced.

---

> P: No, I don’t think we - we just - I think, we just looked at whether we didn’t
completely miss - just the very very eye balling - rough eye balling whether
everything looked sort of ok-ish. But no, no formal checks if the value is like 2%
outside of the original means we do it again until . . . That, we didn’t do. We
didn’t iterate the process. Just did it one time. We wanted to beat the system
but not spend too much time. Basically, that is what happened.

---

>P: No. It was more thinking about which approach would be most hard to detect.
No, it is not difficult to fabricate data, unfortunately. But it is difficult probably
to fabricate data that look like real data. That is not an easy task.

---

<!-- nothing to hide argument88 -->
> really meant for improving science etc. So, my first gut instinct was: I am not
going to fabricate data. That is really bad. But I understand why it is necessary
that we actually increase our ability to detect fabricated datasets that will be
in the benefit - in the long-term benefit of science. That is basically it. And
furthermore, since - although you never know for sure - neither I nor anybody of
my team has ever fabricated data I am much in favor of catching those who did.
We have nothing to fear from a good data fabrication detection system. So, that
is why I ultimately thought, of course, I should participate.

# 1zm

> P: I read a general article in The Guardian, I think, about data fabrication and
I read the book of Diederik Stapel. So, I have a very broad idea of how other
cases were detected but no specific, scientific articles, just popular - like general
audience articles.

---

> P: Yes. I think large effect sizes. Also probably inflated correlations or (?)
correlations, spurious correlations, for instance correlations between standard
deviations, I think, that you wouldn’t expect but are there. And that might
indicate fraud as well. Yeah, what else? I think if you generate it with a statistical

---

> P: Yeah, again, focusing on not too large correlations, not too big effect size, and
making sure that the standard deviations and the reaction times are acceptable,
that they are not too big or too small to be genuine or to be real.

---

> P: First of, I checked the effect size whether it was acceptable. So, I decided
that like a t-value around 2 and a half to 3 or something would be acceptable for
this sample size. And then once I had that, I checked the data in a new Excel
file and I correlated - aeh, I calculated the correlations using Excel and then just

> P: Yeah, just on my impression what would be acceptable. Yeah, I think it can
vary a bit but, yeah, I didn’t really have a concrete idea how big it would be.

---

> But then I noticed that I have some biases myself or that I don’t know exactly
how real data is supposed to look like. And then, yeah, I tried to adjust it but I
was just - I noticed that I just didn’t have enough background or information or
knowledge about how real data structure looks like to be sure that it looks like
a real one. So then, yeah, once I generated it, I always started to worry about
how it should exactly look like.

---

> P: I think it is just worth mentioning that it is quite hard and it also feels that
with some motivation you can do it better, but then it also feels like it is like it
is quite laborious to generate it. So I think that if it becomes so laborious to
just try and generate youre data, it is probably even just easier to collect real
data instead of generating false data for 35 people.

# 2a9

> or tools could look like and I actually have no idea. I know from the tax evasion
literature Benford’s law like the occurrence of different numbers that there is a
certain distribution that you can look at, but I wasn’t sure if that would really
apply to response times data in the Stroop task. So, first, I thought about

---

> in condition two four or five are changed to way higher numbers that conform
with the hypothesis, that obviously looks fake. So, the noisier the data is the
more I would trust it. But that is only for this dumb-detection of ‘hey, this is
exactly the same data only some things are changed’.

---

> P: Yes, that is another thing where I - I guess I could have read up (?) on this.
But I didn’t partly because of the time investment, but also partly because I
think many people who fake this data - they are also not 100% sure of this
methods. So, just adding noise to the data - the - I drew random numbers from
a normal distribution with a mean of zero and added those numbers to the real

---

> P: I guess - since it’s Chris - thanks to Open Science I had a lot of real data
that I could sample from. So, I didn’t have to do it from scratch because thanks
to Open Science there is a lot of real data out there that I can just manipulate.
So, ironically, even though, of course, this is one disadvantage that we should
happily take - one cost - thanks to being more open, I have a lot more real data
to sample from and make my data, make my fake data out of.

---

> P: I mean people are always overconfident. So, I don’t wanna be overconfident
because I don’t know what tools are out there at all. My intuition says it is
going to be hard to detect because it originates from real data but I don’t want
to be too confident.

---

> 