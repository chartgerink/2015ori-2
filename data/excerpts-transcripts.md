# 19e

> it or something. But, yeah, then I thought using a large data set and treat it as
some kind of population and sample from that 25 subjects would be a better
way to maybe avoid detection. Yeah, that was my reasoning.

> looked at the mean of the means, the standard deviation of the means, the mean
of the standard deviations of the - standard deviations. And then, I thought,
well, what else could you guys use to detect fraud and then I thought, maybe
there is something known in the Stroop task about the correlation between
those measures. Probably, there is - I don’t know. So, I also looked at the
correlation between those four measures, right - mean and standard deviation
from the two condition. And then I generated a new 121 sized data set with
those characteristics. So, just in R - I just generated a correlated data set with

> template. Then, manually, I checked for crazy outliers - like I don’t know -
maybe, there were response times of below 100 milliseconds and that would of
course be not possible. But there weren’t any. So, then I decided that that

> P: Not really. I am just very curious as to the methods that other people used.
I mean, I was thinking about, maybe, I should use like an LCA model or an
LBA model to generate data. And I thought: No, I am way overthinking it. It
doesn’t have to be that hard, I guess.

> P: Yeah, because it is, I guess, fun to do something that is really not allowed
in science. And yeah, I mean, I think [REDACTED] also did it and was like

> that sounds like a good challenge. Wonder if I can beat [REDACTED] with
generating data. So, yeah, it seemed like a fun challenge.

> were interested in participating but I think we are too competitive to actually
discuss our methods with each other.

# 1se

>P: No. But I did recruit one of the postdocs.

>P: To discuss how we could do this.

>P: So, I didn’t do this on my own. I recruited additional help.

---

> congruent trial and [?] some 2-300 milliseconds longer. So, there is kind of a
range of plausibility that you can extract from the literature. If the latencies
would be completely off - like by a factor(?) of 5 or 10 - then something surely
must be a miss. So, distribution, absolute values, and the lack of outliers - this
would be, I think, hallmarks of fabrication. That’s what I look for when I get
students’ work. So, that is probably why I come up with these.

---

> P: Of course. We started out with great reluctance not wanting to fake data but
once we accepted the mission we really wanted to make sure that you couldn’t
detect our playing this game. So, yes, we tried to think about how we could
make the data look real.

---

> P: Yeah, we took into account the fact that the absolute latency has a relationship
with the standard deviation. If you are very slow then the standard deviation
will be a little bit longer. So, there was a correlation between the mean and the
standard deviation introduced.

---

> P: No, I don’t think we - we just - I think, we just looked at whether we didn’t
completely miss - just the very very eye balling - rough eye balling whether
everything looked sort of ok-ish. But no, no formal checks if the value is like 2%
outside of the original means we do it again until . . . That, we didn’t do. We
didn’t iterate the process. Just did it one time. We wanted to beat the system
but not spend too much time. Basically, that is what happened.

---

>P: No. It was more thinking about which approach would be most hard to detect.
No, it is not difficult to fabricate data, unfortunately. But it is difficult probably
to fabricate data that look like real data. That is not an easy task.

---

<!-- nothing to hide argument88 -->
> really meant for improving science etc. So, my first gut instinct was: I am not
going to fabricate data. That is really bad. But I understand why it is necessary
that we actually increase our ability to detect fabricated datasets that will be
in the benefit - in the long-term benefit of science. That is basically it. And
furthermore, since - although you never know for sure - neither I nor anybody of
my team has ever fabricated data I am much in favor of catching those who did.
We have nothing to fear from a good data fabrication detection system. So, that
is why I ultimately thought, of course, I should participate.

# 1zm

> P: I read a general article in The Guardian, I think, about data fabrication and
I read the book of Diederik Stapel. So, I have a very broad idea of how other
cases were detected but no specific, scientific articles, just popular - like general
audience articles.

---

> P: Yes. I think large effect sizes. Also probably inflated correlations or (?)
correlations, spurious correlations, for instance correlations between standard
deviations, I think, that you wouldn’t expect but are there. And that might
indicate fraud as well. Yeah, what else? I think if you generate it with a statistical

---

> P: Yeah, again, focusing on not too large correlations, not too big effect size, and
making sure that the standard deviations and the reaction times are acceptable,
that they are not too big or too small to be genuine or to be real.

---

> P: First of, I checked the effect size whether it was acceptable. So, I decided
that like a t-value around 2 and a half to 3 or something would be acceptable for
this sample size. And then once I had that, I checked the data in a new Excel
file and I correlated - aeh, I calculated the correlations using Excel and then just

> P: Yeah, just on my impression what would be acceptable. Yeah, I think it can
vary a bit but, yeah, I didn’t really have a concrete idea how big it would be.

---

> But then I noticed that I have some biases myself or that I don’t know exactly
how real data is supposed to look like. And then, yeah, I tried to adjust it but I
was just - I noticed that I just didn’t have enough background or information or
knowledge about how real data structure looks like to be sure that it looks like
a real one. So then, yeah, once I generated it, I always started to worry about
how it should exactly look like.

---

> P: I think it is just worth mentioning that it is quite hard and it also feels that
with some motivation you can do it better, but then it also feels like it is like it
is quite laborious to generate it. So I think that if it becomes so laborious to
just try and generate youre data, it is probably even just easier to collect real
data instead of generating false data for 35 people.

# 2a9

> or tools could look like and I actually have no idea. I know from the tax evasion
literature Benford’s law like the occurrence of different numbers that there is a
certain distribution that you can look at, but I wasn’t sure if that would really
apply to response times data in the Stroop task. So, first, I thought about

---

> in condition two four or five are changed to way higher numbers that conform
with the hypothesis, that obviously looks fake. So, the noisier the data is the
more I would trust it. But that is only for this dumb-detection of ‘hey, this is
exactly the same data only some things are changed’.

---

> P: Yes, that is another thing where I - I guess I could have read up (?) on this.
But I didn’t partly because of the time investment, but also partly because I
think many people who fake this data - they are also not 100% sure of this
methods. So, just adding noise to the data - the - I drew random numbers from
a normal distribution with a mean of zero and added those numbers to the real

---

> P: I guess - since it’s Chris - thanks to Open Science I had a lot of real data
that I could sample from. So, I didn’t have to do it from scratch because thanks
to Open Science there is a lot of real data out there that I can just manipulate.
So, ironically, even though, of course, this is one disadvantage that we should
happily take - one cost - thanks to being more open, I have a lot more real data
to sample from and make my data, make my fake data out of.

---

> P: I mean people are always overconfident. So, I don’t wanna be overconfident
because I don’t know what tools are out there at all. My intuition says it is
going to be hard to detect because it originates from real data but I don’t want
to be too confident.

# 2f5

> . . . . well, sometimes it was very much interesting to me - when I was doing
on that fluctuations I just found that in some period of time the type of the
numbers I was entering into the system was [?] pretty much repetitive. And the
very time that I detected it that the numbers are going to play around the same
thing, I just tried to - I mean - change the way my fingers were just playing with
the keyboard. It was from up to down instead of - say buttom to up or from left

---

> P: No, but actually, I just described the whole sections. The whole process is
not something very much interesting. I mean when you fabricate the data you
do not have a good feeling. I mean the whole process does not bring any interest.

# 3pl

> P: I did check the correlation between the standard deviation and the mean for
both - the incongruent and congruent parts. And I tried to keep them similar to
the existing dataset that I had. So, for example I saw that the correlation was
lower between the standard deviation of the incongruent trials and the mean of

---

> I: How did you add the error?

... 

> I: And did you do this like by yourself or did you like have a computer program
to simulate how much noise you want to add or . . . ?

> P: No, I did it by myself because I didn’t want any structural changes that could
be detected.

---

> P: Yes. I discussed it because I know that others that I know are also participating
in this. But we also made clear to each other that we didn’t want to say what
we are using. But we were talking about it like what kind of methods would
they use to detect it. None of us could come up with a good way it could be
detected. So, we didn’t share much information in that sense.

---

> P: I felt challanged in the data fabrication and on the one hand I hoped that I
would not be detected but on the other hand I was also afraid that I would be
too good at this. So I am not sure what is the compliment - being good at this
or being bad at this.

# 3wn

> be undetected by your methods. Now, I am assuming that your methods are
good. Which means that you can’t detect like properly generated data, right?
Because otherwise your methods are shit basically, right? So that is like wheream starting. And then, it is very easy because the Stroop effect is something
I

---

> am starting. And then, it is very easy because the Stroop effect is something
that is actually there. It is one of the most replicated things out there. So, my
strategy was to just do the Stroop task myself. And then just generate data
like that. And the only requirement is that we get a significant effect. So, I just
played the Stroop task for 25 times - at different times - and I just take that.
And that’s it. So, that was the whole reasoning. And then - I was thinking
about this - how to generate this ideally. But then the best is just to actually
create the data how it is supposed to be created because then if your methods
are working you won’t detect it as artificially because it is not. So, that’s it.

---

> I: And did you take these characteristics you just mentioned into account when
fabricating the data?

> P: No, that is not necessary because I did not generate the data in an artificial
way.

---

> P: Yeah. So, I went online to find a Stroop task which I could do in my browser.
And I found one. And then I just played it 25 times for the 25 participants.
Then, it was actually 60 trials required but then in this game it was only 40. So
what I did is for each participant I calculated - or for each round of me, basically
- I calculated the ratio of errors and then I scaled up from 40 to 60 trials. And
then again - and the missing ones I sampled from the whole pool of all trials
of all participants. And that is how I scaled it up from 40 to 60. Because that
was the only thing where I was thinking maybe you can find something because
maybe of some variance or something like that that if less variance within person
if you have 60 trials compared to 40 trials. So, I scaled it up. And that is the
only thing what I kind of changed in the data after generating them in a genuine
way. Well, and then I just calculated for each participant the mean in the two
conditions and the standard deviation. And that’s it.

---

> I: So, did you determine in some way whether you were satisfied with the
fabricated data or that they needed to be adjusted?

> P: No, because I knew that the process is a genuine one. So, I didn’t look at
anything.

---

> P: No, I think it was very easy because you were asking to fabricate an effect
which actually exists. It would have been much more difficult - and much more
interesting, I think, for you - to give me the task to create an effect whichactually not there. Because then I couldn’t do it that way.
is

---

> I: Ok. And do you think that your approach to data fabrication will be difficult
to detect as fabricated?

> P: Yeah. It is going to be impossible.

> I: Ok. So you can’t think of any way how it could be detected?

> P: No but like by logic: If you detect it, your method is flawed.

# 82z

> P: So, I think low variance probably and . . . yeah, so, I wasn’t really sure
(?), I think it’s - obviously if maybe effects are too large or maybe if there
aren’t correlations between for instance the mean reaction time and the standard
deviation that are likely to be there in real data.

---

> I: Ok. And do you think your approach to data fabrication will be difficult to
detect as fabricated?
P: Well, I am a bit uncertain about that. So, I know I wouldn’t spot it as
fabricated data but I am not aware of any detection methods or anything other
(?). So, it is kind of hard to get a feel for that, yeah.

# 8nb

> P: I am familiar with general aspects of what went into detecting Diederik
Stapel’s data fabrication and alleged fabrication of Jens Foerster with unrealistic
consistency in results and standard deviations. Yes, so, I am also aware that
individuals are intutitively poor at generating random values. And so I took
that into account in generating data.

---

> P: Yes. So, I took 50 cases from an existing data set where participants had
completed an individual differences scale. I took the - the scale had 21 items
- I took the mean of those 50 participants for the 21 items. For the first 25
participants, I divided the grand mean that I targeted for the congruent group
by the mean of those participants and then I multiplied every individual mean
by that value. And I did the same process for the second group. I took the mean
of all individuals, took the grand mean of that, I divided what I had wanted to
be the grand mean of the incongruent group by that value and I multiplied each
person’s mean score for the scale by that value.

---

> P: Yes. I simply multi- I took the standard deviation of the scale scores for the 21
items on the individual differences measure for every participant and I multiplied
those standard deviations by the average of the two transforms that I had made
for the means so that the variances would be roughly equal between groups. It
wouldn’t be exactly equal but I wouldn’t be multiplying the variance by a larger
number for the incongruent group than I had for the congruent group.

# ez8

> had to take care not to do that. But I thought it would be easier if I just would
go about it as I do - as I would if I was going to fabricate the data myself. And I
thought, it would be the best starting point to at least know what sort of normal
data would be. So, I started looking up the articles, some articles on the Stroop
task, to get a general feel of the response times in both conditions. And I looked

---

> be at least some weird results or outliers. Yeah, what I struggled with more was
that the instructions were not that specific. So, it - a lot of questions came into
my head like what kind of participants are these? Are these young people or
patients or old people or a very varied group? Or was this the vocal version
of the task or a computerized version of the task? I thought I had to at least
try to make the participants a little bit consistent. Because I started out with

---

> When it comes to response times I know at least that people are never quicker
than 150 milliseconds. I just know that from my work, I did [REDACTED]. So,

---

> would make sense. But I actually thought that faking the data would be a lot
harder than just programming the experiment and doing it.

---

> P: I don’t know. I am not sure if I am the only one but I worried a lot about
how many values I should have behind the comma and if they should be similar
or not, if people would look at that or. So, I don’t know, I became very paranoid
of oh my god, what would be - like weird or if I had one outlier, where would it -
would it be participant 24 or 3, would that matter? Like normally, you don’t
even pay attention to these things and now I thought, oh my god, if you really
were committing fraud, this would worry me like can I make participant 25, like
the last one, an outlier or would that be weird? I really - I found that was quite
stressful actually.

---

> P: Well, at some point I thought I was happy with them and then I put them in
the template and then I had the biggest significant effect ever. And I thought
no, this is too much. So, I did try to make it - to then turn it down a little bit.
So, I think it is a little bit with that as well. And I tried to make sure that there

---

> condition and the rest sort of that kind of things. To not make it look too nice.
And I wondered how strong I should make the effect. Because it is quite easy
to make a really significant effect with a really high t-value, so I tried to lower
it a little bit. But then I didn’t know actually what, because a lot of the old
literature doesn’t really report that many t-values anyway. So, I was like hmm,
then I struggled with that. Like ok, because this is such a well-known and such
a strong effect that it could very well be really strong, so yeah, at some point to
be honest I thought you just now have to quit, because they said you should
spend like half an hour on the data fabrication and you are already ongoing like
4. So maybe, you should just let it go and be happy with it now.

---

<!-- nice description -->
> P: There is just so much you have to think about and I thought, yeah like I said
before, I - it would have been so much easier for me to program the experiment
and do it with at least - or do it 25 times myself or with a few friends. I would
have been done within half an hour, I think. To get real data. Because this is
such an easy task to program and to do. It would have been done so quickly
that, yeah - I mean, we had a laugh about this actually, because I have a lot of
friends [REDACTED] and I said, oh my god, it would be just so much easier to
do this for real. And then they said, oh, you would be fabricating your data for
your fabrication experiment, because you have to fake it, so, yeah, that would
be funny. But, yeah, I thought it was hard and stressful in a way, because you
wanna do it well. And you are like, there is so much, I don’t know. Like who
are these people, are they quick or nor, or are they students, are they motivated
to do it well, or are these people who just wanna get it over with or - because I
know these are all things that really influence your data quality. So, I thought
it was, yeah, it was difficult. And then I noticed, I never really thought that
much about my own data in a way. That if you have your own experiment, you
have such a real sense of what it - what makes sense, and you have really strict
things that you think upfront, like what do I think will be an outlier, you know,
who will I take out, but I never thought about my standard deviations in very
abstract way like they should be this high, but so I don’t know, I thought it was
interesting but difficult. And I don’t know why people would do this.

---

> P: Yeah. I thought it was really funny, like a cool challenge. Yeah because I
have many friends [REDACTED], I really try to follow the research that they
do and I also think Chris is doing really cool studies. I think within me, it is
sort of a race like oh this would be so cool, this would be your chance to see if
you could do this or not. And I don’t know, I noticed with my friends that they
had the same response like we thought this would be a really cool game like to
see if you could beat the system to see if you could be really good at faking it.
And to be honest, when I started, I thought I could do this a lot better. Then I
started it and I thought, oh, this is more complicated or more stressful than I
thought it would be. But I don’t know, I think it is really cool. I really hope
that Chris can make an algorithm that can detect it. That would be really cool.
I don’t know I think it is a really nice study.

---

> P: No. It was - I mean sometimes I would want to ask like how did you do this,
but I thought no you have to do this individually, otherwise we would ruin Chris’
study and we didn’t want that, so yeah.

> P: But this is the first thing I am going to do when everyone is done. Compare
tactics.

# g2f

> P: I did. So, one idea was to actually take a real data set, but, yeah, I didn’t do
that, because I didn’t think it would actually be helpful. I started by basically

---

> just drawing 25 times 4 samples from normal distributions. But then I well
started using different kinds of distributions. So, other approaches - other
approaches of course would also just basically get the data. Just test myself on
that amount of trials, but that again would not really be fabrication. But, yeah,
that’s pretty much it.

> P: Because I thought it might be interesting for you to maybe have like a
potential false positive in their, but then I thought, ok, you can probably find
your own real data sets. So, no, I just, yeah . . .

---

> P: Well, it is actually fairly simple, I would say, what I did. But I mean I looked
at parameters of distributions in existing data sets. Then, I recreated these
distributions and sampled from them. So, I had distributions for the means
and for common variance. And then I was making two joint distributions. And
essentially sampling from those.

---

> P: I think this is an extremely good initiative. Data fraud in science is really
bad considering the importance of the whole [?]. The monetary aspect was very
appealing as well. I did end up spending more time than I thought I would,
but I think it is quite worth it. So, yeah, it is mostly wanting to support the
study and also earning some [?] easy money. And also the exercise itself is a
good practice, I think, to think about distributions and how data works, roughly
speaking, yeah.

# h5w

> you in a minute, but some randomizer here and there, because I thought, well,
if I am gonna do that by hand, it is probably not going to be random enough
and that’s what I learned from these earlier experiences, that people tend to
create patterns that are actually not random enough, basically, and like avoiding
round numbers and those sort of things, because they think that makes it looks
random, but in actuality those numbers do occur, of course. So, I tried to use
some of that by using a - by basically changing values using a random number
generator.

---

> P: No, not really. It was - I have to say, though, that it was much more effort
than I thought. It is almost easier to run the experiment to be honest. If I had
run the experiment on Mechanical Turk, I would have been done more quickly.
Yeah, that is - that surprised me. Also because you are second guessing, so I
am thinking the whole time if I were in their position, I would be looking for
this and that. So, I should make sure I, you know, don’t do these things. That
makes a little, yeah, more work, more intense than I had expected. And also
it feels strange to do it, it feels very odd, almost like it is a criminal act which
normally it is, but . . .

---

> higher standard deviation than a person with a longer reaction time and you
could have a smaller standard deviation. So, I tried to look for that and then I
realized that - but maybe then I am getting ahead of myself - that it would be
difficult for me to produce that by hand and so I used an existing data set and
tweaked it. That is basically what I did. Because I thought there may be all
kinds of hidden regularities that I am unable to see but that you guys will be
able to pick up on. And so, I thought if I use a naturally occurring data set and
I tweak it in a certain way, then it probably still has these regularities that are
sort of hidden to me right now but that you guys will be able to pick up. So,
that was basically my thinking.

---

> so good at that and that will take me more time, but then - and I thought,
you guys will probably be able to detect that. So, it is much better if I use
an existing data set and then manipulate it a little bit. So, I used a data set,
you know, from a Stroop experiment and it had 25 - aeh it had 24 subjects, so
I created an additional one basically taking a mean and a standard deviation
that were somewhat comparable to the rest. And I - for the existing data set, I
computed all the condition means and standard deviations and then they had
21 observations per condition, but you wanted 30, right? So, I thought, well, if
you have more observations, then the standard deviation is going to be lower
per subject, so I took the 21 response times from a condition and then copied
the last 9 ones, then I recomputed the standard deviation so that it became a
little bit smaller overall, not always by the way. And then, I did that for the
first 3 or 4 subjects and then after that I sort of got a sense of ok it is going to
be reduced by 20 or 30 milliseconds to that - or 10, yeah, 10 to 30, so I basically
said, ok, this one had 225, I will make it 220. And this one had 234, I will turn
it into 214 or something like that. And I did that for all the standard deviations.
And so then I had a pattern that looked realistic, I thought, because now I have
25 subjects and I have standard deviations that look reasonable for if you have
30 observations per condition. But then I thought, well g (?), maybe they have
some sort of a program that just looks for these kind of reaction times on the
internet, you know, some search program, so I shouldn’t have an exact match.
And then I basically took the condition means and in Excel I randomly added
or - plus or 5 - plus or minus 5 milliseconds, so, you know, so the difference
between the means would still be fairly realistic for a Stroop task, but if you
did a search, you wouldn’t find these exact numbers online. And then I also
re-ordered the subjects to make it a little bit harder to match it up with the
original file. And that is what I did that one morning. And then a few days ago,
I thought, well, but maybe the condition means are still going to be very close to
the original, because all I did was randomly add or subtract, you know, minus 5
to 5 milliseconds, but on the average you should get pretty much the same means.
So the only difference was the 25th subject that I added, but that didn’t change
the overall mean so much. So, I basically - after having re-ordered the subjects,
there was a different number 25 and I gave that subject longer reaction times,
so 100 milliseconds in each condition, and I also made the standard deviations
a little bit higher. So that that influenced the overall mean so that now if you
guys compute the overall means for my conditions, they are not the same or not,
yeah, highly similar to the ones you can find online. Because I figured, well, one
thing they are going to do maybe is for sure look for regularities in the data or
lack thereof. But something else they might do is just, you know, figure out, oh,
some people may use existing data sets, so we are going to have a program that
just goes through the entire Open Science Framework or something like that. So,
I shouldn’t have perfect matches to the data in there. So, that is why I did that.

---

> things that he looks for. And then I thought, well, you need to have a reasonable
effect size, comparable to what you find in the literature. The relation between

---

> interesting for us to know?
P: I think, well, one thing that occurred to me is even though I am an advocate
of open science, you know people publishing their data or posting their data,
there is a risk of course, because they could be doing what I am doing here. You
know you could basically create some sort of Frankensteinian data set out of
other people’s data sets. That is why I think it is useful that I did it this way,
because it would be very good if you could detect this, because I think this might
be what people will start doing, because they will start thinking, well, it is way
too difficult for me to create my own data set, I would have to, you know, learn
how to simulate in R or whatever and even then I am not completely sure if I
capture everything, why not take an existing data set and tweak it. And pretty
soon, there will be data sets for pretty much every experiment that you want.
So, you can just look at those and that is why I think your detection program
should have something where it would automatically scan all those files on the
Open Science Framework for example and then not look for direct matches, I
guess the way you could find this is if you ordered all the response times and
then you would do a correlation with the ones that I have and the original ones,
then that correlation would be extremely high, maybe higher than you would
normally expect. That would be how I would probably try to approach cases
like this.

---

> P: Yes, more difficult than I thought, because it is a - in fact, it is actually, I
think, a useful exercise, because it gives people a much better sense of what data
should look like. So, when you do your own experiment or, you know, you are
a checking let’s say a data set that a collaborator has produced, you can more
easily detect if there is something weird about it, which could be fraud but could
also be a mistake that somebody made, you know. I have seen it where students
use cut-offs for response times if two standard deviations from the mean, but
then they did it in the wrong column and so they got significant effects where I
would not have expected effects and I looked back and I could find the error in
the data file basically because so they had the cut-offs for the means but I said
also compute the medians and the medians showed no effect, but the means did.
And then I thought, oh, there is something wrong with how they did the means
and I think those are the sorts of things you become more sensitive to when you
do an exercise like this. So, I actually thought it was useful to do it.

---

> P: Ok, well, that sounded like a very interesting project that is kind of a counter-
intuitive thing to do, but I think in the long run it will be very useful, because
even though, you know, there are some fraud cases that have become public,
I know of at least 2 or 3 other cases that have not become public and that
probably will never become public, so if I already know 5 or 6, then you know
there must be dozens and dozens. And so, that makes me worried, so I think if
we have good software that allows us to detect fraudulent data then that will be
a benefit to the field. And like I said, when I was doing this, I realized that the
disadvantage of having open data is that there will be many more data sets for
people to work with, to harvest so to speak, to create their own data set so the
better the techniques we have to detect these things, you know, the more the
field will benefit. So, that was my - I was intrigued and I thought it might be
very useful

# h65

> reasoning process during the preparation time a bit?
P: Yeah, so my first instinct was to generate the data using some kind of a model
like a model participant. That’s what I went for in the end. And how - the other
things I considered was doing it by hand - and just filling in values by hand -
or using data I have lying around and change it around a bit. So, using real
data, but then just, yeah, re-using it basically. Those were kind of the three
approaches I have considered.

---

> P: Well, the reason is that I thought, this would be the most difficult to detect,
because I basically - I generated data that should be very similar to how we
make statistical models of data, so I think that is very difficult to distinguish
from real data. And I also think this would be the least susceptible to, yeah,
me making manual errors, repeating numbers or something like that, some kind
of repeating pattern that I would do as a person, right. Yeah, so that is why I
chose for this. I think this would be the hardest to detect.

---

> P: What would to me make data look more fabricated is especially if you have
25 participants if all participants show the same direction of the effect. So if
the within-subject effects are very similar to each other that would to me signal
that someone has been fabricating data. Or repeating numbers, like repeating
patterns of numbers, because it is so unlikely that you will find 498.8 multiple

---

> means and standard deviations per participant. I think it is quite hard to tell if
the data is genuine. I would be more convinced if you had trial-by-trial - because
we were supposed to simulate 30 trials - if you had the trial-by-trial data I think
it would be easier to see – for me at least- if the data looks genuine. So, raw
data would be the answer to that for me.

---

> something like that I would say this not - you did something - I would assume
that someone did something wrong rather than fabricated their data, but, you
know, that would indicate to me that something is up - if the magnitude of
the effect isn’t ok. And the raw data, yeah, if, I don’t know, the amount of
participants doesn’t match up and that kind of stuff like, yeah, I think there is -

---

> P: Well, mainly that - well, what I took into account was that I shouldn’t do
stuff manually, because if I do it manually, you might be able to detect in pattern
how I am entering stuff. So, I thought I should do this with a computer and not
touch it myself at all. And make a very convincing model participant, but don’t
type in the Excel sheet myself. That was my main consideration.

---

> P: Well, yeah, I got all my - yeah, I got my criteria basically from papers. So, if
the Stroop task paper matched my generated data, then I would find that pretty
convincing. Yeah, that basically.

---

> P: [REDACTED]. Yeah, so what I did is that I specified the mean for congruent
condition, which I found from one paper, and then I looked at a couple of papers
what typically the difference in means is for a congruent and an incongruent
condition. And I specified that. And I think the mean here that I used is 672
and I found that the difference between the two conditions is about 50 - is it
milliseconds - yeah, milliseconds on average. And that, yeah basically, that.

---

> P: Yeah, yeah, ok, I kind of just - I just did that, but the standard deviations I
didn’t specify. I used a standard deviation of about 130, I think, and then I said
in the incongruent condition, that’s about 10 higher. So, that’s the values that
it should be centered around, yeah.

---

> P: Yeah, so I - well, for one, if there were no like things that I - some kind of
artefacts. For example, the random noise putting that in was that was a bit
tricky. What it does now is that it determines for each participant a magnitude
of noise between 0 and 1, where 1 is just complete random noise and 0 is perfectly
sampled from a distribution. So, that was a bit tricky to get that right because
if that is correlated with each other, then you don’t get these ellipsoid clouds of
or ellipsoid relation between the mean and standard deviation, but you get these
kind of crosses, you get kind of these axes, these weird shapes that I think would
be very detectable if you looked at it. So, I made sure there was nothing weird
in that. And then once I got the distribution right I looked at papers to see if it
matched the distributions I found in papers. And once it did, I was satisfied.

---

> I: And can you think of ways how it might be detected as fabricated?

> P: Well, I was thinking - well, the fact that you mentioned a random number
generator worried me slightly, that you might find some kind of pattern in that,
that you could find the seed of it somehow. I have no idea but, yeah, so that
would be – that’s worrying me now a little bit. I didn’t consider that. And what
else could you do? I am thinking that maybe there is some kind of information
in the Excel sheet if that makes any sense - if it has been copy-pasted into it or
something like that all in one go, that would be a bit suspicious. I don’t know if
you could retrieve that somehow. But other than that, no.

# hsu

> indicators was the - I should (?) say the linearity in the effects that were reported
that were a bit suspicious. So based on this, I was convinced that right from the
start I should use a simulation approach to simulate my data rather than just -
or to simulate my data with a program like R for example rather than coming
up with a data on my own. Because I thought I myself would be too - how shall
I say - too non-random in coming up with things. So, then I myself would have

---

> and the incongruent trials and the Stroop task and each subject got 30 trials. So,
I thought, ok, I must rather than just coming up with the means and standard
deviations that I need to report for the incongruent and congruent ones, I should
simulate the data in a higher level data meaning simulating all the trials for
each individual in the both conditions. So, that was the first consideration.
The second consideration was how - when - after - while simulating the data,
how can I actually blur my simulation as much as possible and make my own
influence in the simulation procedure as small as possible. So, this is why I
added the second layer of randomness to this where I said, ok, I need - I want
to give a range of possible values that I want to use for my other simulation
then, so this is why I had a first random draw on saying, ok, what - if you have
(?) a range, from this range take for each individual one value that falls into
the range and simulate data based on this value. And then as a third step in
the simulation, I said, ok, how can I add noise to this and again there are this
step-wise procedure I had. First, a range of noise parameters and then I drew
[...]
most obvious - or I thought one of the most obvious things that you could detect
my fabrication with is if I just have unrealistic reaction times for the Stroop
task. So, basically, I don’t know my domain and just fabricate data and get a
significant difference, but just completely off from what Stroop task shows. And

---

> P: So, the - I would say data that has some strange things in it is more - data that
have some random - how should I say this - some strange observations in them
are more likely to be genuine, this is what I would say. So, if you got data that
[...]
incongruent trials. Because you have some participants for which there maybe is
a small difference or it goes the opposite direction, so this [?] some apparently
counter-intuitive stuff going on in your data, this is for me what makes good
fabricated data.

---

> ideally I should have accounted for that as well, which I did not do explicitly -
or not do at all. This is something that I should have done, I think. The other
thing is that I could also have done (?) about a realistic effect size that I would
want to yield with my fabricated data that should ideally also be realistic (?)
of what is commonly found in the Stroop task, which I didn’t do either. I did
[...]
The - ideally you could have said, ok let’s say meta-analyses showed that the
just say the mean Stroop effect is for Cohen’s f effect size of I don’t know .15,
then ideally I could have said, ok, generate data that yields an effect size in the
difference between congruent and incongruent trials approximately in that range
of the meta-analytical findings. But I didn’t do that either.

---

> participants?
P: Yeah. So, let me just check, I can maybe walk through this script quickly.
I came up with some random - with some indications of what the mean could
be for the random generation. And how I did it is I came up with a plausible
maximum for congruent trials, a plausible minimum for congruent trials, same
for incongruent trials. Then I said, these - I did this for the means and standard
deviations of the normal distributions that I would later simulate my data
with. And with those maximum and minimum values I created a sequence with
certain steps in it and from the sequence I drew randomly - for each participant
I drew one number for 30 congruent trials, one for the 30 incongruent trials,
based on this number, the mean and the standard deviation I generated by two
random processes reaction times for the congruent and incongruent trials for
each participant. After I had done that, I added noise to the data, so I had -
for each participant I had a participant ID, I had the trial type, and a reaction
time. And then I added noise to this and I again generated - came up with two
different ranges of how to generate the noise for the data, so I had - I came up
with the variables I called noise-congruent- and noise-incongruent-trials which
consisted of - here of a sequence from 5 - from specific values you can look up (?)
from which I drew random one number. That number I then put into a random
Poisson generator, generated 1000 observations from a Poisson distribution with
a lambda parameter of the noise value, then I multiplied this randomly with
+1 or -1 to add or subtract from the reaction time as a noise parameter and
then added this to the raw reaction time. So, I did this for congruent and
incongruent trials and then the only other steps were that I then had for each
participant 30 observations, 30 trials congruent, 30 trials incongruent, and the
according reaction times and then I aggregated this for the means and the
standard deviations.

---

> observations really what the p-value or again effect size should look like. Because
effect size I didn’t include this at all. And there were other cases where the
t-statistic was so high and accordingly the p-value was very, very small and
I thought, no, this is maybe too obvious. So, I tried to find a middle ground
between not just significant but also not being super significant - if - yeah in a
way this is what I would say.

---

> P: Well, yes, so I think there is some irony in it. I would say at least the - what
I would expect the quite extensive length that I went through for fabricating my
data set I thought why would someone fabricate the data if it takes so long, I
could just collect the data myself, right. So, if I would have just programmed the
[...]
with all the fabrication steps. So yes I would say it is difficult compared to just
collecting the data, but this, I would say, depends on the way how you fabricate
the data. In my case, I tried to put (?) a lot of randomness in it for all the
[...]
in my head, this would probably be much easier than what I did. And then it
would be easier compared to collecting the data. But in my case I would say
you might just as well have collected the data.

---

> this might actually be the K.O. criterion for my data from the start - to look at
does this person actually know their domain. So, have I actually come up with
some values that are realistic for Stroop task? I hope to counter this by looking
up what values could be reasonable for a Stroop task, but this would be the
first thing that I thought, ok, this is how they could detect my fabrication and I
hope to have countered this. The second thing is what I have mentioned before

# jgg

> P: I think, yeah, that we have got some - that we have all got some patterns in
which we like to work. And I think that if you find some patterns in fabricated
data that you might see that it is fabricated. For example, if participant 1, 2, 4
is a mean time of 10, 20, 30, 40, yeah, that is a pattern and I think that might
be fabricated because I think people like to work in patterns.

---

> P: Yeah, I think if data is highly significant it might be fabricated because, yeah,
in real life data is not that significant most of the time.


---

>P: Yeah, I tried to avoid patterns in my fabricated data. And as I said before, I
have looked up the average time for one trial and I have tried to take the average
for the majority of my participants with 1 or 2 outliers, yeah.

---

> P: Ja, I have started up looking like information about the mean total amount of
time for the entire Stroop task in the congruent condition. And I think it might
be - I have [?] it here somewhere - oh yeah, yeah, it was between 6 seconds and
15 seconds or something, so I thought, yeah, that might be great. Then I have
looked into my patients but they were patients of 70, 80 so I thought they may
be a bit too high for an average participant and then I have went to a website in
which you could yourself do a Stroop task of 30 trials and I did it 4 times and I
came like 12 seconds, 13 seconds, 15 seconds for 13 trials. And then I divided
it by 30 and, yeah, that’s when I tried to - well, that is an average of - I don’t
know exactly but maybe half a second per trial. And then I decided, well, then
I am going to randomly type some numbers between 300 milliseconds and 800
milliseconds for each participant so that I’ve got a lot of mean, yeah, second
trial things (?) and then I went to SPSS because I saw that there were actually
a lot of participants scored between - how could I say - 300 milliseconds and
500 milliseconds in a large population. So, I have changed a couple of numbers
so that the propotions were as equal as the website displayed for the general
population.

---

> P: Yeah. Maybe, I have. I saw that my p-value was very low, very significant.
But I decided to let it be as it was because I think the Stroop task and the
Stroop effect is a highly significant effect. So, I think in a real data set, yeah, it
might look like this, ja.

---

> P: Yeah, yeah, it is very difficult. I talked - yeah, before my colleague [?] said
that it might be easier to actually find 25 participants and conduct the Stroop
task than fabricate the data, yeah. It’s difficult, yeah.

# jmq

> P: I did feel a bit excited or nervous during the data fabrication. And I thought
about it. I thought it is probably for three reasons. One is, it felt a little bit
wrong. Just, you know, the idea of creating your own data. But also because I
wasn’t sure whether I was allowed to use real data. It didn’t really say so in the
task. But also it is like - it is not simulating and (?) actually creating new data.
And three, because of the competetive element, it was also more exciting than
just, you know, doing some random stuff.

# nbu

> P: Yeah, because I felt that I could have done a better job if I had more statistical
knowledge. So, now, I feel that I just did something and maybe it is very stupid.
I don’t know. So, I was a bit insecure but it was also kind of uncomfortable
because you were doing something that was not right. But it is an experiment
so when I named the data files like ‘data fabrication experiment’ it felt better
already but it still was a bit weird. So, it was difficult and weird.

# o2f

> P: Yeah, not very extensively, but I thought - I just thought about how natural
data would look, that there would be some outliers, for example, that it wouldn’t
be perfect or maybe too significant or too predictable. So, I thought about that
and tried to remind that all the time when I fabricated the data.

---

> P: Yeah, I think it would look fabricated if it is all too neat and too like wishful
thinking or too perfect - if it looks too perfect, then I think it would be suspicious.
[...]
And I thought,
maybe if you have a lot of - maybe people have a preference for certain numbers
when they write it down or try to avoid double numbers or numbers like 100 or
200, because they think, no that is too obvious, but in a real world situation the
chances are very likely that a number like that would be there. So, I also tried
to correct for that by sometimes putting like 117 or something or even 200, I
think, is in the - something like that.

---

> P: When I wrote everything down, I tried to correct myself by looking at how
many times I had the same end number and starting number and I put in on
purpose some outliers and some, yeah, numbers that maybe some other people
would try to avoid like 400 or something.

---

> P: Yeah, it was more difficult than I expected. The means I didn’t think were so
difficult but the standard deviations were really difficult, I think.

---

> P: That I really had to dive into statistics and how it actually outworked to come
up with realistic numbers. And even though I used real data as an inspiration,
I am still not sure if the spreading of the standard deviations is right. So, like
the standard deviation of the standard deviation. I do not know if I did that
correctly.



# ojh