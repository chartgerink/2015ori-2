# 19e

> it or something. But, yeah, then I thought using a large data set and treat it as
some kind of population and sample from that 25 subjects would be a better
way to maybe avoid detection. Yeah, that was my reasoning.

> looked at the mean of the means, the standard deviation of the means, the mean
of the standard deviations of the - standard deviations. And then, I thought,
well, what else could you guys use to detect fraud and then I thought, maybe
there is something known in the Stroop task about the correlation between
those measures. Probably, there is - I don’t know. So, I also looked at the
correlation between those four measures, right - mean and standard deviation
from the two condition. And then I generated a new 121 sized data set with
those characteristics. So, just in R - I just generated a correlated data set with

> template. Then, manually, I checked for crazy outliers - like I don’t know -
maybe, there were response times of below 100 milliseconds and that would of
course be not possible. But there weren’t any. So, then I decided that that

> P: Not really. I am just very curious as to the methods that other people used.
I mean, I was thinking about, maybe, I should use like an LCA model or an
LBA model to generate data. And I thought: No, I am way overthinking it. It
doesn’t have to be that hard, I guess.

> P: Yeah, because it is, I guess, fun to do something that is really not allowed
in science. And yeah, I mean, I think [REDACTED] also did it and was like

> that sounds like a good challenge. Wonder if I can beat [REDACTED] with
generating data. So, yeah, it seemed like a fun challenge.

> were interested in participating but I think we are too competitive to actually
discuss our methods with each other.

# 1se

>P: No. But I did recruit one of the postdocs.

>P: To discuss how we could do this.

>P: So, I didn’t do this on my own. I recruited additional help.

---

> congruent trial and [?] some 2-300 milliseconds longer. So, there is kind of a
range of plausibility that you can extract from the literature. If the latencies
would be completely off - like by a factor(?) of 5 or 10 - then something surely
must be a miss. So, distribution, absolute values, and the lack of outliers - this
would be, I think, hallmarks of fabrication. That’s what I look for when I get
students’ work. So, that is probably why I come up with these.

---

> P: Of course. We started out with great reluctance not wanting to fake data but
once we accepted the mission we really wanted to make sure that you couldn’t
detect our playing this game. So, yes, we tried to think about how we could
make the data look real.

---

> P: Yeah, we took into account the fact that the absolute latency has a relationship
with the standard deviation. If you are very slow then the standard deviation
will be a little bit longer. So, there was a correlation between the mean and the
standard deviation introduced.

---

> P: No, I don’t think we - we just - I think, we just looked at whether we didn’t
completely miss - just the very very eye balling - rough eye balling whether
everything looked sort of ok-ish. But no, no formal checks if the value is like 2%
outside of the original means we do it again until . . . That, we didn’t do. We
didn’t iterate the process. Just did it one time. We wanted to beat the system
but not spend too much time. Basically, that is what happened.

---

>P: No. It was more thinking about which approach would be most hard to detect.
No, it is not difficult to fabricate data, unfortunately. But it is difficult probably
to fabricate data that look like real data. That is not an easy task.

---

<!-- nothing to hide argument88 -->
> really meant for improving science etc. So, my first gut instinct was: I am not
going to fabricate data. That is really bad. But I understand why it is necessary
that we actually increase our ability to detect fabricated datasets that will be
in the benefit - in the long-term benefit of science. That is basically it. And
furthermore, since - although you never know for sure - neither I nor anybody of
my team has ever fabricated data I am much in favor of catching those who did.
We have nothing to fear from a good data fabrication detection system. So, that
is why I ultimately thought, of course, I should participate.

# 1zm

> P: I read a general article in The Guardian, I think, about data fabrication and
I read the book of Diederik Stapel. So, I have a very broad idea of how other
cases were detected but no specific, scientific articles, just popular - like general
audience articles.

---

> P: Yes. I think large effect sizes. Also probably inflated correlations or (?)
correlations, spurious correlations, for instance correlations between standard
deviations, I think, that you wouldn’t expect but are there. And that might
indicate fraud as well. Yeah, what else? I think if you generate it with a statistical

---

> P: Yeah, again, focusing on not too large correlations, not too big effect size, and
making sure that the standard deviations and the reaction times are acceptable,
that they are not too big or too small to be genuine or to be real.

---

> P: First of, I checked the effect size whether it was acceptable. So, I decided
that like a t-value around 2 and a half to 3 or something would be acceptable for
this sample size. And then once I had that, I checked the data in a new Excel
file and I correlated - aeh, I calculated the correlations using Excel and then just

> P: Yeah, just on my impression what would be acceptable. Yeah, I think it can
vary a bit but, yeah, I didn’t really have a concrete idea how big it would be.

---

> But then I noticed that I have some biases myself or that I don’t know exactly
how real data is supposed to look like. And then, yeah, I tried to adjust it but I
was just - I noticed that I just didn’t have enough background or information or
knowledge about how real data structure looks like to be sure that it looks like
a real one. So then, yeah, once I generated it, I always started to worry about
how it should exactly look like.

---

> P: I think it is just worth mentioning that it is quite hard and it also feels that
with some motivation you can do it better, but then it also feels like it is like it
is quite laborious to generate it. So I think that if it becomes so laborious to
just try and generate youre data, it is probably even just easier to collect real
data instead of generating false data for 35 people.

# 2a9

> or tools could look like and I actually have no idea. I know from the tax evasion
literature Benford’s law like the occurrence of different numbers that there is a
certain distribution that you can look at, but I wasn’t sure if that would really
apply to response times data in the Stroop task. So, first, I thought about

---

> in condition two four or five are changed to way higher numbers that conform
with the hypothesis, that obviously looks fake. So, the noisier the data is the
more I would trust it. But that is only for this dumb-detection of ‘hey, this is
exactly the same data only some things are changed’.

---

> P: Yes, that is another thing where I - I guess I could have read up (?) on this.
But I didn’t partly because of the time investment, but also partly because I
think many people who fake this data - they are also not 100% sure of this
methods. So, just adding noise to the data - the - I drew random numbers from
a normal distribution with a mean of zero and added those numbers to the real

---

> P: I guess - since it’s Chris - thanks to Open Science I had a lot of real data
that I could sample from. So, I didn’t have to do it from scratch because thanks
to Open Science there is a lot of real data out there that I can just manipulate.
So, ironically, even though, of course, this is one disadvantage that we should
happily take - one cost - thanks to being more open, I have a lot more real data
to sample from and make my data, make my fake data out of.

---

> P: I mean people are always overconfident. So, I don’t wanna be overconfident
because I don’t know what tools are out there at all. My intuition says it is
going to be hard to detect because it originates from real data but I don’t want
to be too confident.

# 2f5

> . . . . well, sometimes it was very much interesting to me - when I was doing
on that fluctuations I just found that in some period of time the type of the
numbers I was entering into the system was [?] pretty much repetitive. And the
very time that I detected it that the numbers are going to play around the same
thing, I just tried to - I mean - change the way my fingers were just playing with
the keyboard. It was from up to down instead of - say buttom to up or from left

---

> P: No, but actually, I just described the whole sections. The whole process is
not something very much interesting. I mean when you fabricate the data you
do not have a good feeling. I mean the whole process does not bring any interest.

# 3pl

> P: I did check the correlation between the standard deviation and the mean for
both - the incongruent and congruent parts. And I tried to keep them similar to
the existing dataset that I had. So, for example I saw that the correlation was
lower between the standard deviation of the incongruent trials and the mean of

---

> I: How did you add the error?

... 

> I: And did you do this like by yourself or did you like have a computer program
to simulate how much noise you want to add or . . . ?

> P: No, I did it by myself because I didn’t want any structural changes that could
be detected.

---

> P: Yes. I discussed it because I know that others that I know are also participating
in this. But we also made clear to each other that we didn’t want to say what
we are using. But we were talking about it like what kind of methods would
they use to detect it. None of us could come up with a good way it could be
detected. So, we didn’t share much information in that sense.

---

> P: I felt challanged in the data fabrication and on the one hand I hoped that I
would not be detected but on the other hand I was also afraid that I would be
too good at this. So I am not sure what is the compliment - being good at this
or being bad at this.

# 3wn

> be undetected by your methods. Now, I am assuming that your methods are
good. Which means that you can’t detect like properly generated data, right?
Because otherwise your methods are shit basically, right? So that is like wheream starting. And then, it is very easy because the Stroop effect is something
I

---

> am starting. And then, it is very easy because the Stroop effect is something
that is actually there. It is one of the most replicated things out there. So, my
strategy was to just do the Stroop task myself. And then just generate data
like that. And the only requirement is that we get a significant effect. So, I just
played the Stroop task for 25 times - at different times - and I just take that.
And that’s it. So, that was the whole reasoning. And then - I was thinking
about this - how to generate this ideally. But then the best is just to actually
create the data how it is supposed to be created because then if your methods
are working you won’t detect it as artificially because it is not. So, that’s it.

---

> I: And did you take these characteristics you just mentioned into account when
fabricating the data?

> P: No, that is not necessary because I did not generate the data in an artificial
way.

---

> P: Yeah. So, I went online to find a Stroop task which I could do in my browser.
And I found one. And then I just played it 25 times for the 25 participants.
Then, it was actually 60 trials required but then in this game it was only 40. So
what I did is for each participant I calculated - or for each round of me, basically
- I calculated the ratio of errors and then I scaled up from 40 to 60 trials. And
then again - and the missing ones I sampled from the whole pool of all trials
of all participants. And that is how I scaled it up from 40 to 60. Because that
was the only thing where I was thinking maybe you can find something because
maybe of some variance or something like that that if less variance within person
if you have 60 trials compared to 40 trials. So, I scaled it up. And that is the
only thing what I kind of changed in the data after generating them in a genuine
way. Well, and then I just calculated for each participant the mean in the two
conditions and the standard deviation. And that’s it.

---

> I: So, did you determine in some way whether you were satisfied with the
fabricated data or that they needed to be adjusted?

> P: No, because I knew that the process is a genuine one. So, I didn’t look at
anything.

---

> P: No, I think it was very easy because you were asking to fabricate an effect
which actually exists. It would have been much more difficult - and much more
interesting, I think, for you - to give me the task to create an effect whichactually not there. Because then I couldn’t do it that way.
is

---

> I: Ok. And do you think that your approach to data fabrication will be difficult
to detect as fabricated?

> P: Yeah. It is going to be impossible.

> I: Ok. So you can’t think of any way how it could be detected?

> P: No but like by logic: If you detect it, your method is flawed.

# 82z

> P: So, I think low variance probably and . . . yeah, so, I wasn’t really sure
(?), I think it’s - obviously if maybe effects are too large or maybe if there
aren’t correlations between for instance the mean reaction time and the standard
deviation that are likely to be there in real data.

---

> I: Ok. And do you think your approach to data fabrication will be difficult to
detect as fabricated?
P: Well, I am a bit uncertain about that. So, I know I wouldn’t spot it as
fabricated data but I am not aware of any detection methods or anything other
(?). So, it is kind of hard to get a feel for that, yeah.

# 8nb

> P: I am familiar with general aspects of what went into detecting Diederik
Stapel’s data fabrication and alleged fabrication of Jens Foerster with unrealistic
consistency in results and standard deviations. Yes, so, I am also aware that
individuals are intutitively poor at generating random values. And so I took
that into account in generating data.

---

> P: Yes. So, I took 50 cases from an existing data set where participants had
completed an individual differences scale. I took the - the scale had 21 items
- I took the mean of those 50 participants for the 21 items. For the first 25
participants, I divided the grand mean that I targeted for the congruent group
by the mean of those participants and then I multiplied every individual mean
by that value. And I did the same process for the second group. I took the mean
of all individuals, took the grand mean of that, I divided what I had wanted to
be the grand mean of the incongruent group by that value and I multiplied each
person’s mean score for the scale by that value.

---

> P: Yes. I simply multi- I took the standard deviation of the scale scores for the 21
items on the individual differences measure for every participant and I multiplied
those standard deviations by the average of the two transforms that I had made
for the means so that the variances would be roughly equal between groups. It
wouldn’t be exactly equal but I wouldn’t be multiplying the variance by a larger
number for the incongruent group than I had for the congruent group.

