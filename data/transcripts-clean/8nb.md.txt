No.
8.
I am employed in the section of social psychology in the department of experimental and applied psychology but I would probably best describe myself as in between social psychology, personality psychology, and health psychology.
No.
I have been a participant in a pilot study that used the Stroop task when I was a PhD student. And I have seen demonstrations of the Stroop task but that's it.
I only use SPSS at least once per week.
With peers being general academic colleagues in my department, an 8.
Ok. I will say an 8.
2. I have high confidence in your ability to detect it.
I actually fabricated it a few times. I did it a few different times. Each time that I did it - and in fact I actually did it again today to keep better records and notes - each time I did it it only took me about 10 minutes but there (?) were single session fabrications.
3.
Yes.
Yeah, I would say probably 60 in total. The first time I thought about it quite a bit more and the second two times I just replicated what I had done with better notes and records.
2.
I did a little bit of review of meta-analyses in the Stroop task.
20-30 minutes.
No.
No.
Yes.
Yes.
I am familiar with general aspects of what went into detecting Diederik Stapel's data fabrication and alleged fabrication of Jens Foerster with unrealistic consistency in results and standard deviations. Yes, so, I am also aware that individuals are intutitively poor at generating random values. And so I took that into account in generating data.
I thought about simulating data in R, but my R skills are pretty poor. And I realized it would have taken me several hours to do that and I didn't think that I could invest that amount of time. Maybe, that answered your question. Can you repeat the full question?
Yes.
Ok. I thought about generating data in R from two distributions with the means that I had estimated based on the meta-analyses. I rejected that based on the time investment it would have taken me. I thought about just coming up with numbers out of the top of my head. I also rejected that because I thought that those numbers would be very artificial. So, I did a bit of a compromise in that I took real data that I had observed with a completely different variable and I transformed that in an - manipulated the values, created new values based on those to match with the means and standard deviations that I was hoping to get.
No.
Excessive similarity in standard deviations or variances between two groups. Too little variation compared to what under (?) real data we should expect in terms of a Stroop task. Excessive similarity in values between the two conditions. So if there was simply a linear transform of group 1 relative to group 2 - I just added a value of 5 in milliseconds to everyone in group 2 and (?) everyone in the incongruent relative to the congruent task. Those are some of the top of my head.
Not other than not violating those.
Only to the extent that I decided not to fabricate it of the top of my head and instead relied upon data that I collected for another context that did not have those types of biases that I might introduce.
I don't fully understand the question.
No.
I simply relied upon having used real data as a baseline that I would then linearly transform. And since the real data that I originally had were not fabricated, I thought that by transforming those I would avoid some of the pitfalls of - that might be detected based on me coming up with values.
Yes. I based estimated means on what I had seen in meta-analyses. And so I differentially transformed the two blocks of 25 individuals so that their means were what - not exactly what meta-analyses had indicated - congruent versus incongruent means should be - but approximately.
Nothing that comes up since I thought about this this morning.
No.
Yes. So, I took 50 cases from an existing data set where participants had completed an individual differences scale. I took the - the scale had 21 items - I took the mean of those 50 participants for the 21 items. For the first 25 participants, I divided the grand mean that I targeted for the congruent group by the mean of those participants and then I multiplied every individual mean by that value. And I did the same process for the second group. I took the mean of all individuals, took the grand mean of that, I divided what I had wanted to be the grand mean of the incongruent group by that value and I multiplied each person's mean score for the scale by that value.
Yes. I simply multi- I took the standard deviation of the scale scores for the 21 items on the individual differences measure for every participant and I multiplied those standard deviations by the average of the two transforms that I had made for the means so that the variances would be roughly equal between groups. It wouldn't be exactly equal but I wouldn't be multiplying the variance by a larger number for the incongruent group than I had for the congruent group.
Not with a lot of thought. It was something that was readily available, that I have used in a lot of my research.
Not entirely. I did do a few other modifications. I switched the highest value - I initially got a p-value of about .02 and I reduced the p-value to below .01 by replacing the largest value in milliseconds in the congruent category with the smallest value in the incongruent category. So, I tinkered with the data a little bit further.
I eyeballed to see if there were any variances that looked extremely low or extremely high that might be suspiciuos but I didn't systemetically compare them to a [?] variances that I would expect.
No. I didn't look at a histogram or boxplot of the fabricated results. I just eyeballed them.
Same answer as for (?) the previous question.
I went through the whole process three times but I did not specifically look to see if I had unique mean-and-standard deviation combinations. I briefly eyeballed the whole set of 50 values and I don't believe that I saw any identical means or standard deviations.
It was mostly just that I had not taken the kind of careful, detailed records that I would want to give you in case you were asking for the notes.
The first time I have just been thinking about the way that I wanted to do this and working through it without taking detailed notes. The second time I did take notes of the process, but when I looked over them this morning, I realized that I had not referenced the data set that I had drawn them from and specific cases that I may have switched - information that I thought that you may want - so I briefly did it again and took better notes.
No.
No.
Yes.
Yes, yeah. Transformed and then two values switched between groups, but yes. And differently transformed for the two groups, yeah.
No.
Only that I didn't - I didn't use any additional procedures to mimic real noise because I assumed that the real noise would be apparent in the original data file which is - yeah - I didn't have the - I didn't have the ability to do that quickly or the time to learn how to do something to generate random noise for this procedure. So, I assumed that by taking and transforming real data there would be random noise inherent to it.
Yes.
These are not.
Yes.
No.
Yes. Yes, I think that - I have trust that whatever process you guys are using to detect fabricated data would require a lot of sophistication to circumvent.
Yeah. I mean I think that I probably could have done a better job of - well, first of all, I didn't take into account anything related to the number of trials that you discussed (?) - I thought briefly about what the consequence of the number of the trials would be on the variance that I put in in the two groups. And I didn't take that into account at all. I am not sure that that actually would influence the variance of the two. I also wasn't sure abot different variances between congruent and incongruent for the Stroop task. I didn't look into that. And it seems plausible that there could be more variance in incongruent because there might be greater individual differences in how the incongruency is processed. I didn't model that. And yeah, I am sure that there is other things that if I thought further, I could potentially try to neutralize some of the ways that these data might look fabricated or not correspond with expectations. But ultimately, I do have quite a bit of confidence in data [?] that you are going to be pretty good in detecting these things unless so much effort is put into fabricating data that researchers would be better off just collecting the data.
Wow, that is an interesting question. I guess I have a few thoughts on that. One is that I would guess that you guys are using an algorithm that will be equally difficult for different tasks - in terms of human hours to doing (?) - I assume that most of the effort is in developing an algorithm that can be implemented accross different data sets with equal effort. I would guess that - well, depending on who has answered this call - so I don't know if you got a representative sample of academics to answer this or if only got people with a higher degree of quantitative knowledge. If you got a representative sample, I think that my approach would probably be - will more closely correspond with real data than average. If you got a sample of highly quantitatively sophisticated respondents, I guess that it would be in the lower half.
Yeah, I would refer back to previous answers. I don't know if variances are expected to be systematically different accross the two conditions. I suspect that they might be but I didn't look into it. Yeah, I don't know if there is some consequence of the number of trials that I didn't take into account in transforming the standard deviations. I am actually not even entirely sure - I didn't think about whether the linear transform for the means from the original values should also correspond with the linear transform of the standard deviations. So, I might have systematically over- or underestimated the standard deviations based on the real data that I transformed.
I generally support initiatives of detecting data fabrication. I think that is important. So, I am happy to contribute data points to you. I thought it would be fun. I used to teach statistics when I was a PhD student - just basic ANOVA and regression classes - and I had to "fabricate" - I am using the air quotes for fabricate - example data sets so that we could have easy hand calculations for computing sums of squares and F tests by hand with small data sets. And so, yeah, I had to make up those numbers to make them good for instruction purposes. And I am kind of nostalgic of times when I did that. It was a lot of fun to teach those classes. And so I thought, yeah, I could use a similar approach to fabricate a data set for this project.
Only the professor I share an office with. We got the email invitation at the same time. And he expressed being too busy to participate, I believe. And I said that this sounds like fun and a worthwile initiative and that I was going to do it.
No, no one helped me
No.
No. I should say that I am happy to provide the original data and the notes that I took in case you want to reproduce what I did.
