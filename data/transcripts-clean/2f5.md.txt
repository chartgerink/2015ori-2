Yes.
Well, my field is about psychology of language. It's a type of interdisciplinary field which could fit into both cognitive psychology and experimental psychology.
No because Stroop task has nothing to do with language things.
Yeah. Well, actually, I knew what the type of the task was because when I was doing some practice with E-Prime software, one of say the exercises put (?) in E-Prime [?] was to design a type of Stroop task. So, I was fully aware of what the task is like and the underlying theories behind that task. Yeah, I mean, it was not the first time that I heard about the Stroop task. I was familiar with it although it has nothing to do with language.
Not a Stroop task. But data out of reaction time studies, yes. But also it is also a type of reaction time study. But, yeah, I have done many analyses with (?) reaction time studies.
It is (?) SPSS.
Well, actually, I suppose there is nothing about SPSS that I cannot [?]. I mean I am fully aware with all the statistical analyses which it can be done via SPSS. And also further than that, when it is - what's it - the post-analysis (?) and SEM.
But like - 10. Because I can run SPSS very well and all the statistical analyses there are fine with me.
Well, I think as a fabricated data, I think it could be a bit challenging for someone who is - who looks at it as a real data to understand that it is fabricated.
Well, I suppose, I will give it number 9. Yeah. 
And I will put 1 a score for any expert who does might have an idea in what [?] it could be detected that it is not a real data - set of data.
No, I did it in whole day. But I did not just leave my computer. I mean, I was just stuck (?) to it for a whole day until - from the time I started inserting (?) the data from the time the data was inserted (?) into that Excel file it took a whole day.
If you want to know in how many settings, in one setting.
Well, the pure defined (?) time? I mean, if I exclude everything that I just was doing in the middle - say having a coffee or ... - I think, something between 7 to 8 hours.
Well, how do you mean by effort? If it is just thinking about how it could be manipulated so that in a way it cannot be detected - if you put - if you translate that effort into mental, I mean, into thinking process, well, that means that I would put the highest degree of that effort. But if you mean how much difficult it was for you to think about it that it could be manipulated, well, it was not that much challenging.
Well, this question is a bit tricky because it could point to two different things. In terms of, I mean, the thought process, I was fully concentrated on that. I mean, whole my thinking was devoted to the way it could be manipulated. But at the same time, if you ask me, was that a challenging thing to do, I would say not. It was not challenging, but I had - this is [?] just took whole my thinking - I mean concentration - because I just did my best to fabricate it in a way that it might seem genuine. It was a, I mean, deep thought process, but at the same time not very much challenging.
No. What type of preperation do you mean?
No, I did it just on my experience out of SPSS because I have done many analyses by SPSS. 
Yeah, I just relied on my own experience and how it could be manipulated in a way that it might look genuine.
No, I did not. I just fully relied on my own experience on what real data set can look like. And then based on that I manipulated. Because - well, actually, to begin with I didn't - I used a real set of data set for one person out of a reaction time study in which I knew that it had two conditions - like congruent and incongruent. Another (?) study of my own - one section of it also had two conditions in which the difference between two conditions were statistically significant. I used the data for - I mean, for one of the participants from my own study, put it into the software, and then tried to play around it. I mean, for that particular subject that I got from my data base it - as I said it had two conditions like that congruent and incongruent and I knew that it is going to be statistically different between these two conditions. So, the way I played around it for the imaginary 25 participants was to play around that - I mean, real one sample of data so that the type of difference between those two conditions are kept with some fluctuations - ups and downs - but the same or I thought I did it in the same way, the same type of fluctuation has been put in the second condition so that the real essence is kept there, I mean, that statistical significance is kept there and it is not damaged.
How do you mean by other approaches?
No, I just tried to stick to that real, one sample real set of data from one sample which was genuine and I looked into it the way - the fluctuations - I mean around that, ups and downs, might seem genuine and at the same time kept in both conditions.
The timeline? You mean how much it took for me to do it?
Because I said it is about 7 to 8 hours.
Oh, sorry I just lost my track. Could you please repeat your question again?
What characteristics might lead you to detect that a set of data is fabricated .... well, sometimes it was very much interesting to me - when I was doing on that fluctuations I just found that in some period of time the type of the numbers I was entering into the system was [?] pretty much repetitive. And the very time that I detected it that the numbers are going to play around the same thing, I just tried to - I mean - change the way my fingers were just playing with the keyboard. It was from up to down instead of - say buttom to up or from left to right so that more variety of the numbers are being kept there but the thing that I detected it myself when I wasn't aware of it when I looked at the data I thought the repetition of numbers. Numbers were much like each other and, yeah, I mean, for example, more in 6 or 7 rows consecutively I had lots of 8s, lots of 7s, lots of 6s.
That is the very thing that I have used. I used a real set of data from one sample that the data, I mean, was genuine and it had two conditions, and I knew beforehand that the data was, I mean - of course, it is not meaningful when there is only one subject and to make a comparison between two conditions, if, I mean, these two are statistically significant because after one it is not meaningul statistically to run an analysis but it is within an, I mean, it could just example within a sample in which the two conditions were statistically significant. That's how I based that fabrication - based on the real data of that one sample.
That means that - I just modeled this fabrication based on one sample of real data.
Pardon me.
Well, for example, [?], I mean ... what is it in English - let me think ... dispersion, yeah, I mean, the way data is dispersed, and I mean that standard deviation. And then - yeah, these are the things that could hint about the data is a good one or not. Of course, it - I had different several sets of data about - it is very difficult - I think it is very difficult to detect if someone has done it very much carefully. Otherwise, the person is really a statistician. But if someone has done it carefully, I think it is very difficult to find out.
Yeah because the way I did that fluctuations was in a way that, for example, the output is not going to be the same for two participants because it was ... I mean the probability of having exactly two outputs for two or three subjects is very much, I mean, low. The possibility is very much low. The way I did that fluctuation I was very much - I mean - careful about doing in a way that the mean and standard deviation out of, for example, these imaginary 25 participants for not 2 of the imaginary participants are going to be the same thing.
...
May (?) yes. Because, actually, you wanted it to be something meaningful, I mean, that could support that Stroop effect, yes? Yeah, I took it into consideration.
Again, by looking at a real set of data, how those dispersion is there, and running the analysis by SPSS to see how they might (?) look like each other.
Yeah, I think I answered previously. I just based - made the comparison of the dispersion and yeah standard deviation and everything. Make comparative, I mean, looking into real set of data and then when (?) I fabricated it.
How do you mean?
For both, of course. Yeah, because - and it sometimes in the middle I just did the analysis with software to see how this time that standard deviation looks like, how the mean looks like.
Well, as I said, I just based everything on a real set of data.
Yeah.
Tried to have everything out of it. If it is ok - because it was something done by me and I was fully aware of the whole process of it. For example, I had run the experiment with 86 participants and I also had another study run by participation of 24 participants. I just looked into the way they might just have some overlaps to look like a real set of data.
Well, actually, I mean that the standard deviation comes from the mean, doesn't it? It is very much related to that mean.
Because it shows how much, I mean, the unit that is dispersed from that mean. For example, plus 1, 2, 1, minus 1. They are all really related.
Yeah.
Well, it was also good to look online about the sets of fabricated data if I could find something on the net (?) and to see how they could be detected by a statistician, yeah.
Yeah, another thought for you: As I told you, the way I just set it based on a real set of data I tried to make the fluctuations really genuine as a real set of data. And the very - I mean, the [?] ability that could be detected with different participants in a real set of data but at the same time in a way that the differences between these two conditions are kept statistically significant.
Yeah, I said - for example, I had 24 participants, each one 60 trials, 30 in each condition, and when I was putting the data into SPSS, first I used a real set for, for example, participant number one and based on that real set for 60 trials I just copied and pasted into the whole 60 trials for the second one but with fluctuations that I was very much - I mean - concentrated on the way I am doing that fluctuations because sometimes in the middle I just [?] to the data to see how standard deviation and mean looked like.
Yeah.
Yeah, exactly.
One sample from a real data set. For example, participant number one in here - the data from participant number one is the real data.
For both conditions. For condition - for example - congruent and incongruent and based on that I just played around it.
I don't get you (?), I am sorry.
60 trials.
For each trial.
Yes.
Got (?) the mean.
Yeah, exactly.
Yeah, exactly.
Yeah, yeah exactly.
Yes, yes exactly.
Well, actually, exactly I mentioned it. Just looking at the way the mean out of - I mean - the whole 60 trials for number 1, separately number 2, number 3, number 4 participant the way they look like a real set of data.
Yes, because I was very much cautious that - because of that repetition within (?) using the numbers that I - I just - I was not aware of it, I was doing it unconsciously, really. But I have no idea where is that - during, I mean, at that time I was inserting the data I just found that there are going to be some repetitions between the numbers, not necessarily the same number, but the same sets of number. For example, the frequency of 8, 9, 0, and 3 were being very much - I mean - more than the other sets of numbers and it was eye-catching.
I just changed those combinations, kept - I mean that there was a fluctuation with another sets of numbers so that the variability in using different numbers - I mean and the numbers which I am referring to is in terms of 10 millisecond - you know what I mean.
Because the range might be between 700 and 800, yeah, it doesn't show the variability but what I am pointing to is about even 1 millisecond or 10 millisecond - within 10 milliseconds. 
Yeah because - Yeah, I just answered before.
Well, I have no idea. Because, as I said, just in the middle of it, I just ran the analysis to see if it seems ok or not. But I have no idea how many times I did it. Because it was pretty repetitively done.
I have no idea how many times. Perhaps 7 or 6.
SPSS.
It is a good idea to do it. This is why (?) I just found out that there will be some repetitions in using the numbers. That is why I just detected in the way I was inserting the data myself. It was good but no, I just did it - the way [?] detected that there are going to be some similarities and some repetitions but I just tried to modulate to - I mean - to change it manually.
Yeah, yeah.
Number 1 is - has the real data, yeah.
Yes, exactly.
No, but actually, I just described the whole sections. The whole process is not something very much interesting. I mean when you fabricate the data you do not have a good feeling. I mean the whole process does not bring any interest. But the way that I have (?) tried to have variety of numbers - not I mean the way repetitions - those repetitions were very much eye catching even for me. And the way I just based it on a real - one sample dat - real data.
Eh, well actually, I spend about 7 or 8 hours. That (?) difficulty in terms of the duration, yes, because I was very tired at the end of it.
I suppose, yes. Because at least the output to me seems good.
I have no idea because this is the first time that I have fabricated a set of data. I know many things about SPSS but I do not know many things about detecting fabrication in data. So, to me, it seems good. But actually the one who is expert in detecting any type of data fabrication should have an opinion on that. Well, if you ask me, I say I am very well - I am very good in doing different statistical analyses with SPSS - very much complicated ones are fine with me. I can run SEM very easily. I can run many different things via SPSS. But this is my first time doing data fabrication. I have not - I have never done it before. So if you ask me, do you think this set of data is very much genuine, looks very much genuine, if you ask me I would say, yes, because the output I have tried to model it after a genuine set of data. But this question is what - and I would be very much happy if we could be informed about the way we have done. Do you think it would be possible to inform me that, for example - I mean, the very expert who specializes in detecting such things how much, say from numbers 1 to 10, has rated the degree of, I mean, the way it looks genuine. Do you think you would inform me of that?
Yeah, because actually the very file that I gave you, it does not have any signification of my name - eh indication of my name - so how do you know which data set is mine?
Ok, ja.
No actually, because as I said I am not an expert in that.
I have no idea what tricks are there. Really, I don't know.
Perhaps to test myself, to test myself, and to understand ... and also to learn because later on I should also be able to understand the way data fabrication can go on. Well, it doesn't mean that for learning everything we always (?) have to go through with it but at least you should have some hunches about what that's look like. So, at least I should have done it once earlier myself to see how it works so that later on I could be able to understand if someone has done data fabrication or something.
Yes, yes.
Well, actually, the very second that I needed to have a data set at least with two conditions and which the statistical significance between two conditions is meaningful.
No, never.
No, no.
[?]
Well, eh, it is very interesting that some people are just doing research on the way different methods could be developed to understand how data fabrication is done. And if we could also be informed about - I mean the detection of the fabrication in the data I would be very much thankful.
Well, I think I have mentioned everything about it.
