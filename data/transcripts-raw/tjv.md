### Legend
1. [REDACTED] means that the original word/fragment was deleted to ensure the anonymity of the participants.
2. [?] is a placeholder for words/fragments that could not be transcribed.
3. (?) means that the transcriber was not completely sure what the last word/fragment was, but had a guess.
4. Sentences that begin with "I:" were said by the interviewer
5. Sentences that begin with "P:" were said by the participant
 

### Block 1: General Information

I: Ok. Then we will start with the first block. The goal of this block is to get some general information about you. So, the first question is: Are you a PhD Student?

P: Yes.

I: And what is your field within psychology? With field, we mean for instance social or cognitive psychology? 

P: So, more educational psychology.

I: Ok. And did you conduct any experiments including a Stroop task in your career?

P: No. Oh, sorry any experiments or a Stroop task experiment?

I: A Stroop task experiment.

P: I have not conducted a Stroop task experiment.

I: Ok. But like could you describe your knowledge or experience with the Stroop task a bit?

P: So, during my Bachelor and Master education, we did the Stroop task ourselves and we saw it a lot. 

I: Ok. So, you have read some papers about it or?

P: Yes. So, I knew about it from my studies. But I did refresh my memory, I did look up a paper on the Stroop task for this data fabrication.

I: Ok. And which statistical analysis programs do you use at least once a week? Multiple answers are possible. For instance, SPSS, R, Stata, SAS, Matlab, Python, or any other? 

P: Just SPSS and R.

I: Ok. And how would you rate your knowledge of statistics relative to your peers on a scale from 1, extremely poor, to 10, excellent?

P: I would say a 7.

I: Ok. And how confident are you that your fabricated data will go undetected as fabricated? Again on a scale from 1 to 10, where 1 means extremely insecure and 10 means extremely confident. 

P: 6.
 

### Block 2: Timeline of Data Fabrication Process (When?)

I: Ok. Then this is is the end of the first block about general information. Now, we will start with the second block. The goal of this block is to get some information about the timeline of the data fabrication process. So, the first questions is: Did you fabricate the data in one day or spread the data fabrication over several days?

P: So, I started out - you could say I spread it over 2 days, but actually, yesterday when I tried to find the work I originally did, I couldn't find it anymore. So, I started again. So, I did it all in one day.

I: Ok. And how much time do you estimate that it took you to fabricate the data in their entirety? 

P: In total about 2.5 to 3 hours.

I: Ok, and this refers to like - so like you had sort of 2 trials right? So like was this time then that you just mentioned referring to only the second trial or to both trials?

P: Yes, only to yesterday. So, yesterday 2.5 hours and then the first day I think about an hour.

I: Ok. And how much effort do you feel you invested in fabricating the data on a scale from 1 (no effort at all) to 7 (a lot of effort)? 

P: A 6.

I: Ok. And did you prepare in any way before starting to fabricate the data?

P: Yes.

I: And how much time do you estimate you spent on preparing?

P: About 30 to - well, so, on the preparation before I started the process of generating the fabricated data, I spent about 30 to 45 minutes where I found a paper, picked out some tables, looked at response latencies, and made some notes, and thought ok, so, if I would fabricate data, what kind of distribution would I want to sample from, what should be the bounce, things like that, yeah.

I: Ok, and did you read literature on detecting data fabrication?

P: No, I didn't read literature on data fabrication detection. Only on the Stroop task.

I: Ok. Or did you look into previous cases of data fabrication and how they had been detected or so? 

P: No, no. Like I know about the Stapel affair and data fabrication things, but I didn't look at literature about it. No, I don't know how they found it.

I: Ok. And so you said that you like looked at a paper on the Stroop task. And did this preparation influence your approach to fabricating the data?

P: Yes. So, I used - I looked up 3 papers. but I ended up using 2. And in one paper - one paper did several experiments with the Stroop task and they had some tables with means and standard deviations for the response latency in the congruent and incongruent conditions. And so without looking at the rest of the experiment, I looked at all these different tables to get a feel of what is kind of the minimum mean and what is kind of the maximum mean of the response latency in the two conditions, what is about the size of the standard deviations for these two conditions in each one. And the other paper had some nice figures with distribution of the response time latency, so it made me decide kind of that the standard deviation for the incongruent condition should be a little bit larger than the standard deviation for the congruent condition. That is, what I got out of looking at - and it made me decide kind of what the bounce of the response latencies would be.

I: Ok. Then this is the end of the second block. Do you have any other comments about the timeline of the data fabrication process that you think could be interesting for us to know? 

P: No.
 

### Block 3: Broad Framework of Data Fabrication Process (What?)

I: Ok. Then, we will now start with the third block. The goal of this block is to get some information about the broad framework of the data fabrication process. So, the first question is: Could you name specific characteristics that would make data look fabricated or more fabricated in your opinion?

P: So, if the means and standard deviations look a lot alike or if there is like one mean and a super small standard deviation, I would say that would be suspect. [REDACTED] but those kinds of clues would be what I would look for: striking similarities.

I: Ok. And could you name specific characteristics that would make data look genuine or more genuine in your opinion?

P: No.

I: Ok. And did you take these characteristics you just mentioned into account when fabricating the data?

P: Yeah, so I did because I - for each respondent I sampled from - I took a sample from a different population mean and standard deviation. So, the first thing I did was generate kind of the setup so that I would have for each person a different underlying population mean.

I: Ok. So, you tried this to avoid the striking similarities?

P: Yes.

I: Ok. And did you take into consideration relations in the data other than the Stroop effect itself? 

P: No. So, I did try to kind of google what the correlations within a person would be between the conditions, within-person variability, but my R knowledge is not good enough that I could take that into account in the sampling design. So in the end, yeah, I did spend a couple of minutes looking into whether I could take that into account, but then I thought, this is going tol take too much time. I don't think I can - That would take me a good other day to figure out how to sample that while taking the within-person relations into account. So, I didn’t do that.

I: Ok and what criteria did you use to determine whether you thought your fabricated data would go undetected? 

P: So, I didn’t think about – so, on purpose, I didn’t try to create a mean myself. So, first - I just tried to randomly sample as much as possible, because I thought if I myself start to fill in means, I am gonna go 50 above, 50 below, it is gonna be systematic. So, that is why I used R.

I: Ok. And did you have specific criteria for the means and the standard deviations?

P: Yes, I did. So, I used – it is in the R code as well which I provided – yeah, so I had means for the congruent condition the underlying population means were between 500 and 700 and for the incongruent condition between 650 and 900 and then the standard deviations for the congruent condition, I sampled them from between 70 and 130 and for the incongruent condition between 100 and 160. So, those were my starting values and then for each person - so for each person, I combined these 25 sampled means and standard deviations per person. And then for each person, I sampled from a normal distribution with basically the first mean of the congruent condition and the first standard deviation of the congruent condition, I sampled 30 observations. And then for the incongruent condition, I sampled with the second incongruent mean and standard deviation and I also sampled 30 observations and then I just computed the mean and the standard deviation and then I repeated that for everyone.

I: Ok. And in hindsight, are there things you think you should have paid specific attention to while fabricating the data? 

P: I would say the correlations between - the correlations within person.

I: Ok, and ...- yeah, ok, then this is the end of the third block. Do you have any other comments about the broad framework of the data fabrication process that you think could be interesting for us to know?

P: I don't think so.
 

### Block 4: Specific Steps of Data Fabrication Process (How?)

I: Ok. Then, we will now start with the fourth block. The goal of this block is to get some information about the specific steps of the data fabrication process. So, the first question is: Could you indicate what steps you took to fabricate the means for the participants?

P: Yeah. So I sampled from a uniform distribution 25 population means for the congruent condition and 25 population means for the incongruent condition. I used those values that I sampled to sample 30 observations per person in each condition from a normal distribution.

I: Ok and did you like have specific steps for the relationship between the mean - or like between the value of a person in the congruent condition and the incongruent condition or did you like get these values independent from each other?

P: I got the values independent from each other.

I: Ok. And could you indicate what steps you took to fabricate the standard deviations for the participants? 

P: So, I also generated some population standard deviations by sampling from a uniform distribution between two sets of values. So for the incongruent condition, I sampled the standard deviations from a uniform distribution between 100 and 160. And then for each participant, I used those as the standard deviation to sample all of their standard deviations. I guess, I don't know (?).

I: Ok. And did you repeatedly fabricate data until you were satisfied with the results? 

P: No, I only did one run. 

I: Ok.

P: Yes, because I thought if I start to look at it and tweak it, I am introducing systematic - I was afraid I would introduce some systematic miss (?) so I thought I just let R do the work.

I: Ok. But did you determine whether you were satisfied with the fabricated data?

P: I mean I looked at it and sometimes I thought, well that is kind of an extreme mean, so I don't know if that is good, but then I didn't do anything with that thought. So, I did look at the results.

I: Ok. So did you try to inspect whether the fabricated data looked weird or? 

P: No, no.

I: Ok. Or whether the fabricated data looked genuine or something?

P: No, so here I have these random draws and then I computed the mean and the standard deviation. I did that for every person. I didn't put it underneath each other and I didn't inspect it in any other way

I: Ok, and how many different mean-sd combinations did you fabricate before getting to the final fabricated dataset?

P: So because I - so, I used unique means and standard deviations for every person in every condition.

I: Ok, but so like when you like after you finished your procedure, you had like one result that you copied to the template.

P: Yes.

I: And you didn't like change it afterwards?

P: No, no, I didn't change it, no. I round it.

I: Ok. And besides the supplied spreadsheet, did you use any other computer programs to fabricate the data?

P: I used R.

I: Ok. Did you use a random number generator to simulate data during this study?

P: Yes.

I: Ok and did you use real data during the fabrication process?

P: No. I mean, I was inspired by reported real results. I didn't use real data.

I: Ok and you like used the real data from two different papers, you said, right?

P: Yes. So, I looked at these papers and I did a print-screen (?) of kind of what their figures and tables looked like. And then I decided what to do.

I: Ok. And did you - like for instance check afterwards whether the distribution of your scores were similar to the ...?

P: No.

I: Ok, then this is the end of the fourth block. Do you have any other comments about the specific steps of the data fabrication process that you think could be interesting for us to know? 

P: No.
 

### Block 5: Underlying Rationale of Data Fabrication Process (Why?)

I: Ok. Then, we will now start with the fifth and final block. The goal of this block is to get some information about the underlying rationale of the data fabrication process. So, the first questions is: Did you consider fabricating these data a difficult task to complete?

P: Medium, medium. I didn't think it would be easy, but I also thought it would be doable enough, yes.

I: Ok. And what did you find doable about it or like what did you find difficult about it?

P: I found it - I mean, I think the most difficult thing was deciding on how to approach the task and which kinds of values to use for the means and the standard deviations. I think that was a lot of effort. I have been learning a lot more about R in the last few months, so I was fairly confident that I could do a basic - that I could fabricate it relatively easily in R. So, I wasn't worried about the R part of the actual generation, but it took more effort to decide what kinds of means and standard deviations.

I: So, you said that like deciding about the approach was a difficult step. Did you consider any other approaches how to do it?

P: I mean I considered just thinking of means and standard deviations myself, but I thought that would be too difficult, because then I would think every time, I don't know if this looks too much like the other one. And then - I think that would have - yeah, I started off doing that and then I thought no, it is too difficult.

I: Ok. And do you think that your approach to data fabrication will be difficult to detect as fabricated?

P: I don't know. Honestly, I don't know. I hope so because there is a nice bonus if it is. But I don't think so. Maybe because I didn't take into account everything.

I: Ok. So could you think of ways how people could identify your data set as fabricated?

P: I don't know. I guess maybe if they see the data and if they are like super experts on the topic, then maybe they would see, that is a strange value, like no one has a mean of 900 for a particular condition or something. So, that could be the case. Maybe if they compute correlations and find a super low correlation between people, I mean within-people, maybe that would be an indication that it is not real.

I: Ok. And why did you decide to participate in this study? 

P: Because I thought it would be really interesting. I think the topic is really cool and really relevant. So, I think it is really important to contribute to this kind of research. Yeah, also in doing meta-analysis it is kind of - I really realized the importance of methodological quality in research. And, yeah, so I think it is really important that you guys have data from real fabrications in order to test some methods. Because I think that type of methodological research is not done very often. So, I think it is really relevant.

I: Ok. And did you discuss this study or the fabrication of the dataset for this study with other people?

P: [REDACTED]. So, I definitely did talk to them about [REDACTED]. 

I: Ok, and did these people help you in fabricating the data set?

P: No, it was just a discussion of whether we would and it is interesting.

I: Ok. Then this is the end of the fifth block. Do you have any other comments about the underlying rationale of the data fabrication process that you think could be interesting for us to know?

P: [?]

I: Ok, then this is the end of the interview. Is there anything else you can recall about the data fabrication that you think is worth mentioning?

P: No, I think I explained everything. I don't think there is anything else to add.
