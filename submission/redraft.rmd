---
title: 'Understanding data fabrication: Qualitative Comparative Analysis (QCA) of
fabrication strategies'
author: "CHJ Hartgerink, Jan G Voelkel, Jelte M Wicherts, Marcel ALM van Assen"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  word_document: default
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_depth: 2
csl: ../bibliography/apa.csl
bibliography: ../bibliography/library.bib
---

```{r, echo = FALSE}
suppressPackageStartupMessages(library(QCA))
```

# Introduction 

Cases of data fabrication in research often peak the interest of people in research and beyond, where it speaks to the imagination to understand why, what, or how data were fabricated. 
For reasons why researchers fabricate data, some look at systemic origins (i.e., the bad barrel argument), such as the highly competitive research system [@10.1172/JCI36371;@10.1038/521259a], whereas others look to personality traits that might be predictive of likelihood to commit misconduct [i.e., the bad-apple argument; @10.1038/534173a]. 
What was fabricated (i.e., which results) is often a question that drives scientific integrity committees established to investigate the case in order to correct the scientific record.

How data are fabricated remains puzzling because of incomplete knowledge of cases and lack of first-hand information on data fabrication. 
We have incomplete knowledge about data fabrication strategies because those that are effective at avoiding detection are not available for self-evident reasons (i.e., discovery bias). 
Vice versa, the cases we do know about can teach us what strategies are unsuccessful, but not the range of strategies that potentially are applied.
However, those cases that are discovered often do not result in confessions with explicit descriptions of how they fabricated the data. 
For example, Diederik Stapel confessed to fabricating data and wrote a book about his recollections and interpretations [@stapel2012ontsporing]. 
This is an exceptional case because of the confessive approach he takes. 
However, even in this exceptional case little specific information is available as to how he fabricated data to be of value for research on data fabrication strategies. 
The following excerpt includes a general description of how he fabricated data over the years [@stapel2012ontsporing;@apsstapel]:

>I preferred to do it at home, late in the evening, when everyone was asleep. I made myself some tea, put my computer on the table, took my notes from my bag, and used my fountain pen to write down a neat list of research projects and effects I had to produce ... Subsequently I began to enter my own data, row for row, column for column ... 3, 4, 6, 7, 8, 4, 5, 3, 5, 6, 7, 8, 5, 4, 3, 3, 2. When I was finished, I would do the first analyses. Often, these would not immediately produce the right results. Back to the matrix and alter data. 4, 6, 7, 5, 4, 7, 8, 2, 4, 4, 6, 5, 6, 7, 8, 5, 4. Just as long until all analyses worked out as planned. (p. 167)

Moreover, when scientific integrity committees investigate for data fabrication, rarely will they be able to conclusively state how data were fabricated. 
Some data fabrication strategies might be obvious if the raw data are available (e.g., copy-pasting responses), whereas others will be less obvious (e.g., multivariate modeling of the observed variables for the desired outcomes). 
Moreover, many fabricated results have underdetermined data fabrication strategies (i.e., multiplicity). 
That is, many different fabrication strategies can result in the observed data set, such that differentiating which one actually occurred based on the data is non-trivial [see also @10.12688/f1000research.12584.1].
Additionally, given that odds of data availability decrease each year after publication [@10.1016/j.cub.2013.11.014], methods to learn about how data were fabricated retrospectively by looking at the dataset are increasingly unfeasible even if underdetermination was not a problem. 
Data availability is especially problematic in cases where it takes longer to uncover problems in the first place, such that data are more likely to be unavailable for investigation in cases that go undetected longer.
Second-hand information about data fabrication offers relatively little indication on how data are actually fabricated by researchers due to this underdetermination and increasing lack of data availability.

Hence, first-hand knowledge from controlled settings about how researchers fabricate data is useful to further understand and maybe even improve detection of data fabrication in uncontrolled settings. 
There is unknown variability in how researchers fabricate data, which could result in foregone detection mechanisms if we focus on the limited and preselected knowledge that is available. 
However, we currently do not even know what methods are used and therefore only operate from hunches and inferences from psychology theory [e.g., @Haldane1948-nm] and how often they occur.
As far as we know, only one such study asked participants to fabricate data, but did not investigate how they did so [@10.1136/bmj.331.7511.267].
Qualitative information would provide a first-hand insight into how data are fabricated and could provide fruitful avenues for the development of new statistical tools to detect data fabrication. 

A better understanding of how researchers fabricate data can assist in developing- and validating tools to detect data fabrication. 
Previously, researchers developed methods to detect data fabrication in situ. 
For example, terminal digit analysis was developed as part of the Imanishi-Kari case [@]; testing for excessive amounts of high $p$-values was developed as part of the Fuji case [@]; variance analysis was developed as part of the Smeesters and Sanna cases [@10.1177/0956797613480366]. 
As such, it seems plausible that development of additional statistical tools can be facilitated by detailed descriptions of data fabrication cases. 
Validation of such statistical tools is also benefited by detailed descriptions of data fabrication, when those details are less prone to selection bias. 
By having less biased details (e.g., less detection bias) on how researchers go about fabricating data, it facilitates better contextualized simulations to investigate the efficacy of these statistical tools. 
After all, the results of these simulations are only as good as the set of data fabrication behaviors that are captured in the simulation. 
<!-- Er mist nog 1 superbelangrijke alinea, nl over waarom het zo belangrijk is te weten hoe men data fabriceert.  -->
<!-- Ik zou dit in aparte alinea zetten en oppompen (pompen pompen!) â€“ belang van weten HOE moet duidelijk zijn. Geef ook een voorbeeld of voorbeelden hoe dat dan kan helpen.
Ontwikkelen verschillende technieken te detecteren
 Proces van ontdekken, waarbij deze technieken kunnen worden toegepast; bijv, eerst kijken of er ruwe data is, dan naar of er stimulus materiaal voor handen is, dan naar of er missings zijn (zo niet, dan zegt dat wellicht iets over gebruik pc), dan iets over grootte effect size, etc.
Dit is verrekte lastig, mag ook op het eind gedaan worden! Ik bedoel, in discussie -->

In this report, we qualitatively assess the data fabrication strategies that practicing researchers used to fabricate data in one of our controlled studies [@2015ori-1]. 
Based on the transcripts of the interviews about how participants fabricated data [@10.5281/zenodo.832490], we apply qualitative methods to learn more about characteristics of the observed data fabrication strategies. 
Moreover, we combine the observed data fabrication strategies with results from various statistical tools to detect data fabrication [see also @], in order to assess whether certain data fabrication characteristics cause better or worse detection.

# Methods

We used transcripts of 28 interviews with researchers who we previously asked to fabricate data in a controlled setting [available at @10.5281/zenodo.832490]. 
In these interviews, we asked participating researchers to answer questions separated into five sections. 
Section 1 pertained to general information about the researcher (e.g., frequent programs used). 
Section 2 inquired about the time and days spent on fabricating data (e.g., how many hours spent). 
Section 3 asked the researcher about their general framework with which they fabricated the data (e.g., what makes data look weird according to them).
Section 4 focused on the specific steps taken to fabricate data (e.g., did they use a (pseudo-)random number generator).
Section 5 was about the motivations of the researcher to participate in this study and their general assessment of their performance. All participating researchers consented to the public sharing of their transcripts.

To recapitulate, we previously asked these 28 researchers to fabricate raw data for a Stroop experiment [see Figure 1; @stroop1935]. 
In short, a Stroop experiment is typically a within-subjects experiment with two conditions measuring response times: (1) congruent (e.g., the word 'red' is presented in red) and (2) incongruent (e.g., the word 'red' is presented in green). 
We asked the participating researchers to fabricate response times for 25 participants, such that there was a statistically significant effect between conditions (i.e., a Stroop effect). 
Using these fabricated data ([](https://osf.io/xxxxx), we tested whether statistical methods could help separate fabricated data sets from (assumably) genuine datasets from Many Labs 3 [[https://osf.io/n8xa7/](https://osf.io/n8xa7/); @10.1016/j.jesp.2015.10.012].

![Example of a filled in template spreadsheet used in the fabrication process. Respondents fabricated data in the yellow cells and green cells, which were used to compute the results of the hypothesis test of the condition effect. If the fabricated data confirm the hypotheses, a checkmark appeared. This template is available at [https://osf.io/2qrbs/](https://osf.io/2qrbs/).](../figures/spreadsheet2.png)

In this paper, we take a two-pronged approach to evaluating the transcripts of these interviews. First, we provide qualitative summaries and reflections on each transcript. We include a description of the researcher (e.g., career stage, statistics knowledge) and how they fabricated the data according to the transcript. We also add what we considered noteworthy anecdotes from the interview. Secondly, we systematically compare what characteristics   researchers applied to fabricate the data using Qualitative Comparative Analysis [QCA; @rihoux2008]. The first approach provides us with a more detailed but also less systematic picture of data fabrication, whereas the second approach provides us with a more general and more systematic picture of data fabrication.

## Qualitative Comparative Analysis

In Qualitative Comparative Analysis [QCA; @rihoux2008], qualitative information is deconstructed into characteristics and related to an outcome measure. 
In crisp set QCA, which we apply here, these characteristics are binary [e.g., present v absent;@rihoux2008]. 
Each unique combination of characteristics is regarded as a pattern and is used to assess necessary and sufficient conditions for the binary outcome measure to be present or absent. 
Using the coded characteristics for each unit of analysis (e.g., participants, group), we compile truth tables. 
Table xxxxx depicts a fictitious example of a truth table. 
For each unique combination of characteristics, the range of outcomes is inspected. 
The pattern `0-0-0` (first row) is observed $\geq1$ times and, in this sample, always leads to the absence of the outcome (vice versa for the last row).
The pattern `0-0-1` (second row) is observed $>1$ times and has conflicting (`C`) outcomes; both presence and absence occur with this pattern.
The pattern `0-1-0` (third row) is not observed and therefore has no information about the outcome (i.e., a logical remainder; `?`).
A truth table can subsequently be minimized to determine necessary and sufficient conditions for the outcome to be present or absent.

```{r, echo = FALSE}
data(LC)
LC <- LC[, -(1:2)]
names(LC) <- c(sprintf('char%s', 1:3), 'outcome')
ttLC <- truthTable(LC, "outcome")
ttLC$tt$OUT[2] <- 'C'
df <- ttLC$tt[,1:4]

knitr::kable(df, caption="Example of a truth table as used in crisp set Qualitative Comparative Analysis (csQCA). The outcome measure is the dependent variable, where the various patterns of the characteristics are used to determine under what conditions the outcome is observed. A `?` indicates that pattern was not observed and therefore the outcome is unknown; a `C` indicates that this pattern was observed >1, but that both outcomes occurred, creating a conflict in csQCA.")
```

Based on the interview protocol ([osf.io/xxxx](https://osf.io/xxxx)), we identified five general data fabrication characteristics for our QCA. 
Each unique combination of data fabrication characteristics makes up a data fabrication strategy. 
We limited ourselves to five characteristics, considering that 2^*n* strategies would be possible. 
In other words, we balanced the number of transcripts (i.e., 28) to the number of unique data fabrication strategies possible (i.e., $2^4=16$; $2^5=32$; $2^6=64$). 
We coded whether 
(1) the participant prepared for the data fabrication (e.g., by reading literature on detecting data fabrication); 
(2) the participant used a (pseudo-)Random Number Generator (RNG) in fabricating the data; 
(3) the participant used assumably genuine Stroop data; 
(4) the participant duplicated or transformed data; 
(5) the participant checked the fabricated data for detectibility. 
The first author coded each of these five data fabrication characteristics for all of the 28 transcripts. 
Additionally, we coded ten participant characteristics (e.g., PhD attained, self-reported statistical knowledge; further described at [osf.io/xxxx](https://osf.io/xxxx) and available at [osf.io/xxxx](https://osf.io/xxxx)). 
We note that these data fabrication characteristics are inherently multiplicitous, hence, we do not know how much we will learn from the QCA. 

As outcome measures, we included whether the researcher's fabricated data was detected as such, by taking the results from the three best statistical methods to detect data fabrication [@REF]. 
In our original project, we included XX tests to detect data fabrication and included only the top three here, based on their Area Under the Curve value. 
We could assess these AUCs based on (assumably) genuine data from the Many Labs 3 initiative [@10.1016/j.jesp.2015.10.012]. 
As a result, we included as outcome measures the results of the detection methods based on 
(1) ?
(2) ?
(3) ?
We did not include the other methods, which consisted of (amongst others) X, X, and X. 
<!-- how did we determine this -->
<!-- which methods are this -->
<!-- which methods didn't make it -->

We conducted separate csQCA analyses combining the coded data fabrication characteristics with the outcomes of these statistical detection methods. 
Usually, unique combinations in csQCA with conflicting outcomes are either omitted (comparable to listwise deletion) or additional characteristics are inductively added to resolve the conflicting outcomes [@rihoux2008]. 
Here, we omit conflicting outcomes for analysis and try to qualitatively assess potentially relevant characteristics. 
We do not run additional QCAs because adding additional characteristics quickly increases the state space of unique patterns to 128 (by just adding two) or beyond (512 by adding four). 
If we would add two characteristics it  would result in maximum coverage of `r round((28/128) * 100, 0)`% by our participants; maximum coverage would be `r round((28/512) * 100, 0)`% if we add four characteristics. 

We used the R package `QCA` [@r-core;@10.1016/j.jbusres.2007.01.002] to conduct these csQCAs. 
For each of the three statistical methods to detect data fabrication, we assessed both sufficient and necessary conditions for detection as well as going undetected. 
We minimized the truth tables using the enhanced Quine-McCluskey algorithm [@10.1002/j.1538-7305.1956.tb03835.x]. 
Data are available at [osf.io/xxxxx](https://osf.io/xxxxx); analysis code is available at [osf.io/xxxxx](https://osf.io/xxxxx). 

<!-- https://raw.githubusercontent.com/chartgerink/2015ori-2/c706d77e4d85daa285624028d379d7b9b00bd1d0/submission/manuscript.Rmd -->

# Results

## Qualitative summaries

For each participant, we provide a qualitative summary of the interview with selected quotes. 
These summaries can be read separately from each other and provide detailed descriptions of our participants' fabrication process. 
Each summary is structured into six paragraphs: (1) how did the participant prepare for data fabrication, (2) how much time did the participant spend fabricating data, (3) what was the general framework with which the participant fabricated data, (4) what are the specifics to the data fabrication process, (5) whether the participant checked the fabricated data for detectability, and (6) what was the motivation to participate.
Generally, these are descriptions of blocks 2-5 of the original interviews (see [Methods](#methods) for a summary of the blocks).
In order to remain neutral to the gender of the participant, we address the participant with their pseudonymized ID and with the pronoun 'they'.
We share these summaries here and not in an appendix because this paper focuses on a qualitative evaluation of the data fabrication strategies applied. 
In Table xxxx we present an overview of the participant characteristics with hyperlinks to the raw transcript files [@10.5281/zenodo.832491] and to the summaries provided in this manuscript (only available in the HTML version of this manuscript). 


| ID |  Summary | PhD attained | Stroop experience | Software knowledge | Statistics knowledge | 
| -- | ----------- | ------------ | ----------------- | ------------------ | | -------------------- |
[0jg](https://zenodo.org/record/832491/files/0jg.md) | [Link](#participant-0jg) | No | No | [ matlab, R ] | 8 | 

<!-- https://zenodo.org/record/832491 -->
<!-- Insert table with participant characteristics here -->

<!-- prep -->
<!-- fabrication time -->
<!-- fabrication framework -->
<!-- fabrication specifics -->
<!-- detection -->
<!-- fabrication motive -->

### Participant 0jg

Prior to fabricating the data, Participant `0jg` prepared by investigating existing literature on the Stroop task. The preparation by Participant `0jg` focused on assessing the scales and typical moments (e.g., standard deviations) of the response time measures. This preparation did not span more than half an hour and did not extend to investigating how previous cases of data fabrication were detected ("I thought it was a little bit cheating also to do it [check how data fabrication could be detected]").

Participant `0jg` spent approximately two hours on fabricating the data, on two separate days. The majority of the data fabrication took place on the first day and finished on the second day. Participant `0jg` also indicated that this was "not that much effort" from their part and that additional effort could have been put in by trying to investigate how we might go about trying to detect the fabricated data ("it was too much effort to do it").

In order to make the data look less fabricated and more genuine, `0jg` indicated that the data should not follow the hypotheses or theory absolutely. In their own words, data would look more fabricated "if the difference between the conditions is equal for all participants - or between conditions" and "probably if it is [...] too normally distributed". `0jg` also indicated that the test statistic for which they fabricated data affected the decisions made in the data fabrication process; "I noticed that in the [...] t-test that you did [...] the individual scores donâ€™t matter [...] I didnâ€™t pay too much attention to the size of the individual differences because I thought, [...] this doesnâ€™t really matter anyway".

`0jg` decided to fabricate the data by (1) taking a mean response time for one condition, (2) determine a condition effect, and (3) jitter the mean to get to the scores for individual fabricated participants. As such, "I had a mean of 545 with a difference between the conditions of 125 milliseconds." Subsequently, `0jg` added noise by drawing from a standard normal distribution multiplied by a constant. "I took a difference of 6 meaning that the congruent was 6 lower (so 67) than the incongruent [which was?] 73 as a standard deviation. So I drew from a standard normal distribution and multiplied it with the standard deviation." Effectively, for the congruent condition noise was added by drawing from $N~(0,67)$ and for the incongruent condition drawn from $N(0,73)$.






<!-- detection -->


<!-- fabrication motive -->
As motivation to participate, `0jg` indicated various reasons. The goal of the study was "fun to do" and they found how to fabricate data "an interesting question" to think about.

## csQCA

```{r}

```


<!-- # Discussion -->

# Author's note

All materials used in this project are available at [https://github.com/chartgerink/2015ori-2](https://github.com/chartgerink/2015ori-2) and are preserved at Zenodo. This project was funded by the Office of Research Integrity (ORI-).

# References
