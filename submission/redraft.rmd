---
title: 'Understanding data fabrication: Qualitative Comparative Analysis (QCA) of
fabrication strategies'
author: "CHJ Hartgerink, Jan G Voelkel, Jelte M Wicherts, Marcel ALM van Assen"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  word_document: default
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_depth: 2
csl: ../bibliography/apa.csl
bibliography: ../bibliography/library.bib
---

```{r, echo = FALSE}
suppressPackageStartupMessages(library(QCA))
```

# Introduction 

Cases of data fabrication in research often peak the interest of people in research and beyond, where it speaks to the imagination to understand why, what, or how data were fabricated. 
For reasons why researchers fabricate data, some look at systemic origins (i.e., the bad barrel argument), such as the highly competitive research system [@10.1172/JCI36371;@10.1038/521259a], whereas others look to personality traits that might be predictive of likelihood to commit misconduct [i.e., the bad-apple argument; @10.1038/534173a]. 
What was fabricated (i.e., which results) is often a question that drives scientific integrity committees established to investigate the case in order to correct the scientific record.

How data are fabricated remains puzzling because of incomplete knowledge of cases and lack of first-hand information on data fabrication. 
We have incomplete knowledge about data fabrication strategies because those that are effective at avoiding detection are not available for self-evident reasons (i.e., discovery bias). 
Vice versa, the cases we do know about can teach us what strategies are unsuccessful, but not the range of strategies that potentially are applied.
However, those cases that are discovered often do not result in confessions with explicit descriptions of how they fabricated the data. 
For example, Diederik Stapel confessed to fabricating data and wrote a book about his recollections and interpretations [@stapel2012ontsporing]. 
This is an exceptional case because of the confessive approach he takes. 
However, even in this exceptional case little specific information is available as to how he fabricated data to be of value for research on data fabrication strategies. 
The following excerpt includes a general description of how he fabricated data over the years [@stapel2012ontsporing;@apsstapel]:

>I preferred to do it at home, late in the evening, when everyone was asleep. I made myself some tea, put my computer on the table, took my notes from my bag, and used my fountain pen to write down a neat list of research projects and effects I had to produce ... Subsequently I began to enter my own data, row for row, column for column ... 3, 4, 6, 7, 8, 4, 5, 3, 5, 6, 7, 8, 5, 4, 3, 3, 2. When I was finished, I would do the first analyses. Often, these would not immediately produce the right results. Back to the matrix and alter data. 4, 6, 7, 5, 4, 7, 8, 2, 4, 4, 6, 5, 6, 7, 8, 5, 4. Just as long until all analyses worked out as planned. (p. 167)

Moreover, when scientific integrity committees investigate for data fabrication, rarely will they be able to conclusively state how data were fabricated. 
Some data fabrication strategies might be obvious if the raw data are available (e.g., copy-pasting responses), whereas others will be less obvious (e.g., multivariate modeling of the observed variables for the desired outcomes). 
Moreover, many fabricated results have underdetermined data fabrication strategies (i.e., multiplicity). 
That is, many different fabrication strategies can result in the observed data set, such that differentiating which one actually occurred based on the data is non-trivial [see also @10.12688/f1000research.12584.1].
Additionally, given that odds of data availability decrease each year after publication [@10.1016/j.cub.2013.11.014], methods to learn about how data were fabricated retrospectively by looking at the dataset are increasingly unfeasible even if underdetermination was not a problem. 
Data availability is especially problematic in cases where it takes longer to uncover problems in the first place, such that data are more likely to be unavailable for investigation in cases that go undetected longer.
Second-hand information about data fabrication offers relatively little indication on how data are actually fabricated by researchers due to this underdetermination and increasing lack of data availability.

Hence, first-hand knowledge from controlled settings about how researchers fabricate data is useful to further understand and maybe even improve detection of data fabrication in uncontrolled settings. 
There is unknown variability in how researchers fabricate data, which could result in foregone detection mechanisms if we focus on the limited and preselected knowledge that is available. 
However, we currently do not even know what methods are used and therefore only operate from hunches and inferences from psychology theory [e.g., @Haldane1948-nm] and how often they occur.
As far as we know, only one such study asked participants to fabricate data, but did not investigate how they did so [@10.1136/bmj.331.7511.267].
Qualitative information would provide a first-hand insight into how data are fabricated and could provide fruitful avenues for the development of new statistical tools to detect data fabrication. 

A better understanding of how researchers fabricate data can assist in developing- and validating tools to detect data fabrication. 
Previously, researchers developed methods to detect data fabrication in situ. 
For example, terminal digit analysis was developed as part of the Imanishi-Kari case [@]; testing for excessive amounts of high $p$-values was developed as part of the Fuji case [@]; variance analysis was developed as part of the Smeesters and Sanna cases [@10.1177/0956797613480366]. 
As such, it seems plausible that development of additional statistical tools can be facilitated by detailed descriptions of data fabrication cases. 
Validation of such statistical tools is also benefited by detailed descriptions of data fabrication, when those details are less prone to selection bias. 
By having less biased details (e.g., less detection bias) on how researchers go about fabricating data, it facilitates better contextualized simulations to investigate the efficacy of these statistical tools. 
After all, the results of these simulations are only as good as the set of data fabrication behaviors that are captured in the simulation. 
<!-- Er mist nog 1 superbelangrijke alinea, nl over waarom het zo belangrijk is te weten hoe men data fabriceert.  -->
<!-- Ik zou dit in aparte alinea zetten en oppompen (pompen pompen!) â€“ belang van weten HOE moet duidelijk zijn. Geef ook een voorbeeld of voorbeelden hoe dat dan kan helpen.
Ontwikkelen verschillende technieken te detecteren
 Proces van ontdekken, waarbij deze technieken kunnen worden toegepast; bijv, eerst kijken of er ruwe data is, dan naar of er stimulus materiaal voor handen is, dan naar of er missings zijn (zo niet, dan zegt dat wellicht iets over gebruik pc), dan iets over grootte effect size, etc.
Dit is verrekte lastig, mag ook op het eind gedaan worden! Ik bedoel, in discussie -->

In this report, we qualitatively assess the data fabrication strategies that practicing researchers used to fabricate data in one of our controlled studies [@2015ori-1]. 
Based on the transcripts of the interviews about how participants fabricated data [@10.5281/zenodo.832490], we apply qualitative methods to learn more about characteristics of the observed data fabrication strategies. 
Moreover, we combine the observed data fabrication strategies with results from various statistical tools to detect data fabrication [see also @], in order to assess whether certain data fabrication characteristics cause better or worse detection.

# Methods

We used transcripts of 28 interviews with researchers who we previously asked to fabricate data in a controlled setting [available at @10.5281/zenodo.832490]. 
In these interviews, we asked participating researchers to answer questions separated into five sections. 
Section 1 pertained to general information about the researcher (e.g., frequent programs used). 
Section 2 inquired about the time and days spent on fabricating data (e.g., how many hours spent). 
Section 3 asked the researcher about their general framework with which they fabricated the data (e.g., what makes data look weird according to them).
Section 4 focused on the specific steps taken to fabricate data (e.g., did they use a (pseudo-)random number generator).
Section 5 was about the motivations of the researcher to participate in this study and their general assessment of their performance. All participating researchers consented to the public sharing of their transcripts.

To recapitulate, we previously asked these 28 researchers to fabricate raw data for a Stroop experiment [see Figure 1; @stroop1935]. 
In short, a Stroop experiment is typically a within-subjects experiment with two conditions measuring response times: (1) congruent (e.g., the word 'red' is presented in red) and (2) incongruent (e.g., the word 'red' is presented in green). 
We asked the participating researchers to fabricate response times for 25 participants, such that there was a statistically significant effect between conditions (i.e., a Stroop effect). 
Using these fabricated data ([](https://osf.io/xxxxx), we tested whether statistical methods could help separate fabricated data sets from (assumably) genuine datasets from Many Labs 3 [[https://osf.io/n8xa7/](https://osf.io/n8xa7/); @10.1016/j.jesp.2015.10.012].

![Example of a filled in template spreadsheet used in the fabrication process. Respondents fabricated data in the yellow cells and green cells, which were used to compute the results of the hypothesis test of the condition effect. If the fabricated data confirm the hypotheses, a checkmark appeared. This template is available at [https://osf.io/2qrbs/](https://osf.io/2qrbs/).](../figures/spreadsheet2.png)

In this paper, we take a two-pronged approach to evaluating the transcripts of these interviews. First, we provide qualitative summaries and reflections on each transcript. We include a description of the researcher (e.g., career stage, statistics knowledge) and how they fabricated the data according to the transcript. We also add what we considered noteworthy anecdotes from the interview. Secondly, we systematically compare what characteristics   researchers applied to fabricate the data using Qualitative Comparative Analysis [QCA; @rihoux2008]. The first approach provides us with a more detailed but also less systematic picture of data fabrication, whereas the second approach provides us with a more general and more systematic picture of data fabrication.

## Qualitative Comparative Analysis

In Qualitative Comparative Analysis [QCA; @rihoux2008], qualitative information is deconstructed into characteristics and related to an outcome measure. 
In crisp set QCA, which we apply here, these characteristics are binary [e.g., present v absent;@rihoux2008]. 
Each unique combination of characteristics is regarded as a pattern and is used to assess necessary and sufficient conditions for the binary outcome measure to be present or absent. 
Using the coded characteristics for each unit of analysis (e.g., participants, group), we compile truth tables. 
Table xxxxx depicts a fictitious example of a truth table. 
For each unique combination of characteristics, the range of outcomes is inspected. 
The pattern `0-0-0` (first row) is observed $\geq1$ times and, in this sample, always leads to the absence of the outcome (vice versa for the last row).
The pattern `0-0-1` (second row) is observed $>1$ times and has conflicting (`C`) outcomes; both presence and absence occur with this pattern.
The pattern `0-1-0` (third row) is not observed and therefore has no information about the outcome (i.e., a logical remainder; `?`).
A truth table can subsequently be minimized to determine necessary and sufficient conditions for the outcome to be present or absent.

```{r, echo = FALSE}
data(LC)
LC <- LC[, -(1:2)]
names(LC) <- c(sprintf('char%s', 1:3), 'outcome')
ttLC <- truthTable(LC, "outcome")
ttLC$tt$OUT[2] <- 'C'
df <- ttLC$tt[,1:4]

knitr::kable(df, caption="Example of a truth table as used in crisp set Qualitative Comparative Analysis (csQCA). The outcome measure is the dependent variable, where the various patterns of the characteristics are used to determine under what conditions the outcome is observed. A `?` indicates that pattern was not observed and therefore the outcome is unknown; a `C` indicates that this pattern was observed >1, but that both outcomes occurred, creating a conflict in csQCA.")
```

Based on the interview protocol ([osf.io/xxxx](https://osf.io/xxxx)), we identified five general data fabrication characteristics for our QCA. 
Each unique combination of data fabrication characteristics makes up a data fabrication strategy. 
We limited ourselves to five characteristics, considering that 2^*n* strategies would be possible. 
In other words, we balanced the number of transcripts (i.e., 28) to the number of unique data fabrication strategies possible (i.e., $2^4=16$; $2^5=32$; $2^6=64$). 
We coded whether 
(1) the participant prepared for the data fabrication (e.g., by reading literature on detecting data fabrication); 
(2) the participant used a (pseudo-)Random Number Generator (RNG) in fabricating the data; 
(3) the participant used assumably genuine Stroop data; 
(4) the participant duplicated or transformed data; 
(5) the participant checked the fabricated data for detectibility. 
The first author coded each of these five data fabrication characteristics for all of the 28 transcripts. 
Additionally, we coded ten participant characteristics (e.g., PhD attained, self-reported statistical knowledge; further described at [osf.io/xxxx](https://osf.io/xxxx) and available at [osf.io/xxxx](https://osf.io/xxxx)). 
We note that these data fabrication characteristics are inherently multiplicitous, hence, we do not know how much we will learn from the QCA. 

As outcome measures, we included whether the researcher's fabricated data was detected as such, by taking the results from the three best statistical methods to detect data fabrication [@REF]. 
In our original project, we included XX tests to detect data fabrication and included only the top three here, based on their Area Under the Curve value. 
We could assess these AUCs based on (assumably) genuine data from the Many Labs 3 initiative [@10.1016/j.jesp.2015.10.012]. 
As a result, we included as outcome measures the results of the detection methods based on 
(1) ?
(2) ?
(3) ?
We did not include the other methods, which consisted of (amongst others) X, X, and X. 
<!-- how did we determine this -->
<!-- which methods are this -->
<!-- which methods didn't make it -->

We conducted separate csQCA analyses combining the coded data fabrication characteristics with the outcomes of these statistical detection methods. 
Usually, unique combinations in csQCA with conflicting outcomes are either omitted (comparable to listwise deletion) or additional characteristics are inductively added to resolve the conflicting outcomes [@rihoux2008]. 
Here, we omit conflicting outcomes for analysis and try to qualitatively assess potentially relevant characteristics. 
We do not run additional QCAs because adding additional characteristics quickly increases the state space of unique patterns to 128 (by just adding two) or beyond (512 by adding four). 
If we would add two characteristics it  would result in maximum coverage of `r round((28/128) * 100, 0)`% by our participants; maximum coverage would be `r round((28/512) * 100, 0)`% if we add four characteristics. 

We used the R package `QCA` [@r-core;@10.1016/j.jbusres.2007.01.002] to conduct these csQCAs. 
For each of the three statistical methods to detect data fabrication, we assessed both sufficient and necessary conditions for detection as well as going undetected. 
We minimized the truth tables using the enhanced Quine-McCluskey algorithm [@10.1002/j.1538-7305.1956.tb03835.x]. 
Data are available at [osf.io/xxxxx](https://osf.io/xxxxx); analysis code is available at [osf.io/xxxxx](https://osf.io/xxxxx). 

<!-- https://raw.githubusercontent.com/chartgerink/2015ori-2/c706d77e4d85daa285624028d379d7b9b00bd1d0/submission/manuscript.Rmd -->

# Results

## Qualitative summaries

For each participant, we provide a qualitative summary of the interview with selected quotes. 
These summaries can be read separately from each other and provide detailed descriptions of our participants' fabrication process. 
Each summary is structured into at least five paragraphs: (1) how did the participant prepare for data fabrication, (2) how much time did the participant spend fabricating data, (3) what was the general framework with which the participant fabricated data, (4) what are the specifics to the data fabrication process, and (5) whether the participant checked the fabricated data for detectability.
Generally, these are descriptions of blocks 2-5 of the original interviews (see [Methods](#methods) for a summary of the blocks).
In order to remain neutral to the gender of the participant, we address the participant with their pseudonymized ID and with the pronoun 'they'.
We share these summaries here and not in an appendix because this paper focuses on a qualitative evaluation of the data fabrication strategies applied. 
In Table xxxx we present an overview of the participant characteristics with hyperlinks to the raw transcript files [@10.5281/zenodo.832491] and to the summaries provided in this manuscript (only available in the HTML version of this manuscript). 

```{r echo = FALSE}
df <- data.frame(id, summary, phd, stroop, soft_know, stat_know, confidence)
names(df) <- c('ID', 'Summary', 'PhD attained', 'Stroop experience', 'Software knowledge', 'Statistics knowledge [self-assessed, 1 (worst) - 10 (best)]', 'Confidence going undetected [self-assessed 1 (extremely insecure) - 10 (extremely confident)]')
df <- rbind()
```

<!-- update this table with all characteristics not used in QCA? -->
<!-- add density plots on data? -->

| ID |  Summary | PhD attained | Stroop experience | Software knowledge | Statistics knowledge [self-assessed, 1 (worst) - 10 (best)] | Confidence going undetected [self-assessed 1 (extremely insecure) - 10 (extremely confident)] | 
| -- | ----------- | ------------ | ----------------- | ------------------ | | -------------------- | --------------------------------------------------------------- |
| [0jg](https://zenodo.org/record/832491/files/0jg.md) | [Link](#participant-0jg) | No | No | [ matlab, r ] | 8 | 6 |
| [19e](https://zenodo.org/record/832491/files/19e.md) | [Link](#participant-19e) | No | No | [ r, python ] | 7 | 5 |
| [1se](https://zenodo.org/record/832491/files/1se.md) | [Link](#participant-1se) | Yes | Yes | [ spss, r ] | 7 |  8 |
| [1zm](https://zenodo.org/record/832491/files/1zm.md) | [Link](#participant-1zm) | Yes | No | [ spss ] | 5 | 4 |

<!-- https://zenodo.org/record/832491/files/ -->

<!-- prep -->
<!-- Prior to fabricating the data, Participant `xxx` -->

<!-- fabrication time -->
<!-- Participant `0jg` spent approximately two hours on fabricating the data, on two separate days. -->

<!-- fabrication framework -->
<!-- fabrication specifics -->
<!-- detection -->

### Participant `0jg`

Prior to fabricating the data, Participant `0jg` prepared by investigating existing literature on the Stroop task. The preparation by Participant `0jg` focused on assessing the scales and typical moments (e.g., standard deviations) of the response time measures. This preparation did not span more than half an hour and did not extend to investigating how previous cases of data fabrication were detected ("I thought it was a little bit cheating also to do it [check how data fabrication could be detected]").

Participant `0jg` spent approximately two hours on fabricating the data, on two separate days. The majority of the data fabrication took place on the first day and finished on the second day. Participant `0jg` also indicated that this was "not that much effort" from their part and that additional effort could have been put in by trying to investigate how we might go about trying to detect the fabricated data ("it was too much effort to do it").

In order to make the data look less fabricated and more genuine, `0jg` indicated that the data should not follow the hypotheses or theory absolutely. In their own words, data would look more fabricated "if the difference between the conditions is equal for all participants - or between conditions" and "probably if it is [...] too normally distributed". `0jg` also indicated that the test statistic for which they fabricated data affected the decisions made in the data fabrication process; "I noticed that in the [...] t-test that you did [...] the individual scores donâ€™t matter [...] I didnâ€™t pay too much attention to the size of the individual differences because I thought, [...] this doesnâ€™t really matter anyway".

`0jg` decided to fabricate the data by (1) taking a mean response time for one condition, (2) determine a condition effect, and (3) jitter the mean to get to the scores for individual fabricated participants. As such, "I had a mean of 545 with a difference between the conditions of 125 milliseconds." Subsequently, `0jg` added noise by drawing from a normal distributions with different variances. For the variances, "I took a difference of 6 meaning that the congruent was 6 lower (so 67) than the incongruent [which was?] 73". Effectively, for the congruent condition noise was added by drawing from $N~(0,67)$ and for the incongruent condition drawn from $N(0,73)$.

<!-- detection -->
Upon fabricating the data, the participant checked for (1) too large effect sizes, (2) minimal deviation of the preconceived standard deviations, and (3) odd histograms. As a result, the participant repeated the data fabrication several times, but "did not after each time inspect [the] data thoroughly. I just looked it at [sic] and thought well, just throw in a different one and go ahead and run it again." These checks were off the cuff and not strictly defined, considering the participant noted that they "kind of had it in the back of my mind [to make it look like real data] all the time but no specific steps for it."

### Participant `19e`

<!-- prep -->
Prior to fabricating the data, Participant `19e` indicated to have not done any conscious preparation. "I guess I should have. That is a good idea. But I didn't, no. [...] I read a few meta-analyses on the Stroop effect. Hopefully, that gave me some insight. But no, I did not look at the literature on data fabrication itself." 

<!-- fabrication time -->
Participant `19e` spent approximately three hours on fabricating the data, on a single day. "I just put a block of [...] four or six hours in my calender and just devoted it to this thing." Apparently, the participant took less time (three hours) than planned (four or six hours) and at the same time indicated "it was more difficult than I expected." When asked how much effort they spent on fabricating the data, they indicated "It took some thinking as to what would constitute a good fabricated data set" and that they rated their own effort as 4 out of 7. In other words, it seems like they found fabricating the data more difficult than expected and did not put in much effort, which might explain why the time spent is shorter than blocked in their schedule.

<!-- fabrication framework -->
In order to fabricate data and make them look genuine, `19e` set out to reuse a (supposedly) genuine data set. They indicated that they could "just look at [their] own old data [...] the means and standard deviations or maybe do even the calculations per subject [...] throw some noise over it or something." But they figured they could use "a large data set and treat it as some kind of population and sample from that [...] to maybe avoid detection." Reflecting upon the approach, "the first option [using own data] would be easier [to detect], I guess. [...] I think the approach that I took now [finding a large dataset as an artificial population to sample from] would be less easy to detect."

Participant `19e` indicated several key points that would make data look more fabricated, including duplication and severe deviations in summary statistics. More specifically, they mentioned that "repetition of data [...] would be evidence for fabrication." They also indicated to realize the data should be in accordance with what was previously published; "I think the best indicator for fabrication would be means or standard deviations that would not be similar to what has been found in earlier studies." Beyond that, they considered "cross-correlations between [...] measures" and that "real data is not perfectly distributed."

<!-- fabrication specifics -->
After searching on the Web for `stroop.csv`, Participant `19e` found a large dataset to use as a population to sample from. They admitted they "don't know if it is real" and the dataset contained 121 subjects, which was considered sufficient. For each of those subjects, they computed the means and standard deviations for both the congruent and incongruent conditions across the individual trials. Subsequently, they looked at the distributions of the data ("that looked pretty normal"), the summary statistics of the mean and standard deviations, and the correlation between the means and standard deviations (although they did not specify how). Using these pieces of information, they "generated a new 121 sized data set with those characteristics." From that generated dataset, they sampled 25 subjects once. In other words, the process involved finding a genuine dataset, investigate several summary statistics and their summary statistics, simulate data using that information, and then sampling a subset of that simulated data.

<!-- detection -->

<!-- * "the distribution of the data I did not really take into account because in the sample data set that I found online on many subjects - all the measures looked pretty normally distributed." -->
<!-- * "I just checked the histrograms. But it all looked pretty ok-ish." -->
<!-- * "manually, I checked for crazy outliers [...] maybe, there were response times of below 100 milliseconds and that would of course be not possible" -->
<!-- * "I made sure that my fabricated data set would show the same intercorrelation between all variables. The same mean and standard deviation of means and standard deviation and standard deviation of standard deviations." -->

Upon fabricating the data, the participant checked for (1) odd histograms, (2) outliers, and (3) comparable correlations and summary statistics as the original dataset. They did not check the histograms systematically --- "it all looked pretty ok-ish." For outliers, they "checked for crazy outliers [...] maybe, there were response times of below 100 milliseconds and that would of course be not possible." Additionally, the participant indicated that
> "I compared the characteristics of my data - so, the mean of means and the standard deviation of means, mean of standard deviations, and standard deviation of standard deviations and all the cross-correlations between those measures - and I made sure that they were quite similar to in the real data set that I found."

### Participant `1se`

<!-- prep -->
Prior to fabricating the data, Participant `1se` read one of their own papers on the Stroop effect. "I read one of our own papers on the Stroop effect [...] I needed [...] the means and standard deviations to have a starting point [for fabricating data]." Additionally, they recruited one of their postdocs to help out with the data fabrication. "I didn't do this on my own. I recruited additional help [...] I don't think we discussed it more than twice in total." Participant `1se` was particular in the sense that they first actively rejected participation in the study. "I don't want to participate" they said; "my first gut instinct was: I am not going to fabricate data. That is really bad." After some back and forth about the goals and procedure of the study, they decided to participate after all. "I understand why it is necessary that we actually increase our ability to detect fabricated datasets that will be in the benefit - in the long-term benefit of science."

<!-- fabrication time -->
Participant `1se` spent approximately six hours on fabricating the data, across four separate days. They indicated that they did not spend much effort, estimating it at a 2 on a scale from 1 (minimal effort) through 7 (a lot of effort). This somewhat conflicts their verbal assessment of the data fabrication process; "it is not difficult to fabricate data, unfortunately. But it is difficult probably to fabricate data that look like real data." This could either be taken as that they did not try to fabricate data that looked real, that their recollection minimizes effort, or a variety of other factors. The first seems implausible, because the participant indicated that "if this [fabricating undetectable data] is the mission [...] we gonna [sic] beat them."

<!-- fabrication framework -->
Participant `1se` indicated that data look more fabricated when they fullfill three criteria: (1) too perfectly distributed, (2) implausible absolute values, and (3) no outliers. More literally, they said data look fabricated when "they clearly come from a normal distribution" and "they are devoid of clear outliers". For plausibility of absolute values, an example was given: "If the response latency would be 6 seconds, that would be very silly because the response latency is known to be - I don't know from the top of my head - but 4-500 milliseconds for a congruent trial and [?] some 2-300 milliseconds longer."  These three criteria served as the basis for fabricating genuine-looking data. Criteria one and three seem somewhat contradictory, because Participant `1se` indicated that they wanted to fabricate a datset that was "at least semi-normal but not perfectly normal." However, in a perfectly normal distribution there are by definition outliers, whereas in a semi-normal distribution there could be situations that there are fewer or no outliers.

<!-- fabrication specifics -->
To actually fabricate the data, `1se` simulated data and manually introduced outliers. In order to fabricate real looking data, their "starting points always were realistic value for the Stroop effect." For example, they took "into account the fact that the absolute latency has a relationship with the standard deviation" in order to align the fabricated data with genuine data, drawing from their experience. More specifically, they used the multivariate normal function in R to generate this random data. The starting points for the simulation were "means from one of [their] own studies but [they] squared them to get skew". In order to make the data less perfect (i.e., more weird) they deliberately introduced outliers and deleted things at random. To fabricate the within-person standard deviations, they used a non-central $\chi^2$ distribution, where the mean of the standard deviation was set to ~30% of the mean itself. After rounding all the fabricated values to milliseconds, they copied the values to the template. "But, in all honesty, it was really done by hand and also not written down. So, this was haphazard. I couldn't reproduce it the next time. "

<!-- detection -->
Participant `1se` was confident in the algorithm used to produce the data and did not check for any signs of data fabrication in the resulting data. One of the reasons indicated pertained to time and mental constraints; "We could have done a better job by looking into realistic standard deviations within subjects but we had no time to do that [...] we were just too lazy to do that." They did eye-ball an unspecified set of plots for obvious signs of data fabrication, but included no formal and systematic checks. As a result, there was only one iteration in fabricating data. `1se` indicated they "wanted to beat the system but not spend too much time."

### Participant `1zm`

<!-- prep -->
Prior to fabricating the data, Participant `1zm` indicated to not have taken any concrete steps to prepare. They indicated to "have a very broad idea of how other cases were detected" but no specific insights beyond those available in general audience articles. An explicit mention was made of [an article in The Guardian](http://web.archive.org/web/20180501202718/https://www.theguardian.com/science/2017/feb/01/high-tech-war-on-science), which covered the work by us within which this experiment took place.

<!-- fabrication time -->
Participant `1zm` spent two hours on fabricating the data, across three to four separate days. They indicated to have spent a medium amount of effort (4 on a 1-7 scale), where later in the interview there appeared a conflict in spending more effort, indicating that "that it [fabricating data] is quite hard and it also feels that with some motivation you can do it better, but then it also feels like it is like it is quite laborious to generate it. So I think that if it becomes so laborious to just try and generate youre data, it is probably even just easier to collect real data instead of generating false data".

<!-- fabrication framework -->
In order to make data look less fabricated, `1zm` indicated too have spent attention on effect sizes, correlations, distributions, and acceptable reaction times. 
With respect to effect sizes, they ensured "that they are not too big or too small to be genuine or to be real."
They "also made sure that people with higher reaction times have somewhat larger effects."
With respect to correlations, "mean reaction times should correlate, standard deviations shouldn't correlate or shouldn't correlate highly." 
This was done mostly on self-reported hunches about how these correlations would look like in genuine data.
With respect to distributions, they considered drawing values from a distribution as implausible and too perfect, because genuine data would be messy and skewed. 
No specifications were made as to what were considered acceptable reaction times.

<!-- fabrication specifics -->
Participant `1zm` started fabricating right away and iterated towards an acceptable data set. Initially, they looked up what average reaction times are in a Stroop task to get a reasonable starting point to fabricate data; "that is around 5[00]-600 milliseconds and I used this to generate the reaction times [...] I thought that the standard deviations would [?] be around 100-120 milliseconds." In order to fabricate the data for the incongruent Stroop condition, they copied the reaction times previously fabricated and added "few tens of milliseconds." Note: generating here meant making up the numbers, not using a random number generator. The participant explicitly noted that "sometimes I also made sure that people didn't show the effect." In order to ensure lack of large correlations between the standard deviations, they reverse copied the standard deviations across conditions (i.e., the last 12 SDs of the congruent condition were pasted as the first 12 of the incronguent condition) in order to prevent biased data generation ("I might be biased if I had to generate it myself"). These various steps did not follow each other immediately, but were part of the iterating process of fabricating the data, where "after a while I came back to it to adjust some things." In total, `1zm` indicated to have "generated it [data] once, but then I adjusted it like 5 times."

<!-- detection -->
In this iterating process, `1zm` checked for too large effect sizes, correlations, and self-identified biases in fabricating the data.
For the effect sizes, they decided that "a t-value around 2 and a half to 3 or something would be acceptable for this sample size" and that this would be the demarcation for effective data fabrication. Admittably, they indicated this was arbitrary and based "just on my impression what would be acceptable."
They compared correlations between the summary statistics and "made sure that there wasn't like a large correlations for instance between the standard deviations." If that was the case, they "adjusted the data a bit to decrease the correlation." All in all, they assessed the data fabrication process as "surprisingly more difficult than I anticipated." Notably, they self-reflected on their ability to fabricate data that could pass as genuine: 
>"I noticed that I have some biases myself or that I don't know exactly how real data is supposed to look like [...] I noticed that I just didn't have enough background or information or knowledge about how real data structure looks like to be sure that it looks like a real one. So then, yeah, once I generated it, I always started to worry about how it should exactly look like."
As a result, they weren't able to assess how confident they were in these fabricated data going undetected, where "they are quite genuine but I also have the impression that there might be some easy things in there to catch that it is actually not real data."

## csQCA

```{r}

```


<!-- # Discussion -->

# Author's note

All materials used in this project are available at [https://github.com/chartgerink/2015ori-2](https://github.com/chartgerink/2015ori-2) and are preserved at Zenodo. This project was funded by the Office of Research Integrity (ORI-).

# References
